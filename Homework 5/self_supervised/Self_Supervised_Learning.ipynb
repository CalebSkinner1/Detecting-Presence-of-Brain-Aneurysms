{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0dxUIgrH3QP"
      },
      "source": [
        "# SimCLR on CIFAR10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fur3UY91rhAy",
        "outputId": "7b8cd0a3-4b36-4d65-ebab-81e5e4fc8a38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: thop in /usr/local/lib/python3.12/dist-packages (0.1.1.post2209072238)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from thop) (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->thop) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->thop) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->thop) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->thop) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->thop) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->thop) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "%pip install thop\n",
        "import os\n",
        "import importlib\n",
        "import random\n",
        "from thop import profile, clever_format\n",
        "from getpass import getpass\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# %load_ext autoreload\n",
        "# %autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "44KeVL3FH3QS",
        "outputId": "5cfd0a7e-7be4-47a9-cc74-0098109189c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXtwio7U3mSI",
        "tags": [
          "pdf-title"
        ]
      },
      "source": [
        "# Self-Supervised Learning\n",
        "\n",
        "## What is self-supervised learning?\n",
        "Modern day machine learning requires lots of labeled data. But often times it's challenging and/or expensive to obtain large amounts of human-labeled data. Is there a way we could ask machines to automatically learn a model which can generate good visual representations without a labeled dataset? Yes, enter self-supervised learning!\n",
        "\n",
        "Self-supervised learning (SSL) allows models to automatically learn a \"good\" representation space using the data in a given dataset without the need for their labels. Specifically, if our dataset were a bunch of images, then self-supervised learning allows a model to learn and generate a \"good\" representation vector for images.\n",
        "\n",
        "The reason SSL methods have seen a surge in popularity is because the learnt model continues to perform well on other datasets as well i.e. new datasets on which the model was not trained on!\n",
        "\n",
        "## What makes a \"good\" representation?\n",
        "A \"good\" representation vector needs to capture the important features of the image as it relates to the rest of the dataset. This means that images in the dataset representing semantically similar entities should have similar representation vectors, and different  images in the dataset should have different representation vectors. For example, two images of an apple should have similar representation vectors, while an image of an apple and an image of a banana should have different representation vectors.\n",
        "\n",
        "## Contrastive Learning: SimCLR\n",
        "Recently, [SimCLR](https://arxiv.org/pdf/2002.05709.pdf) introduces a new architecture which uses **contrastive learning** to learn good visual representations. Contrastive learning aims to learn similar representations for similar images and different representations for different images. As we will see in this notebook, this simple idea allows us to train a surprisingly good model without using any labels.\n",
        "\n",
        "Specifically, for each image in the dataset, SimCLR generates two differently augmented views of that image, called a **positive pair**. Then, the model is encouraged to generate similar representation vectors for this pair of images. See below for an illustration of the architecture (Figure 2 from the paper)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token = getpass('Enter your GitHub token: ')\n",
        "# token: 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"
      ],
      "metadata": {
        "id": "QRiGs2fEI6VC",
        "outputId": "0eda556c-87da-423c-cf1a-502518aba5c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your GitHub token: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://$token@github.com/CalebSkinner1/CS540-Statistical-Machine-Learning.git\n",
        "\n",
        "print(\"Current directory:\", os.getcwd())"
      ],
      "metadata": {
        "id": "3hAQkW15MqFA",
        "outputId": "b291b3e9-eb9b-4506-d96c-a3e53c3e58d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CS540-Statistical-Machine-Learning'...\n",
            "remote: Enumerating objects: 299, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 299 (delta 0), reused 0 (delta 0), pack-reused 293 (from 1)\u001b[K\n",
            "Receiving objects: 100% (299/299), 124.07 MiB | 37.69 MiB/s, done.\n",
            "Resolving deltas: 100% (122/122), done.\n",
            "Current directory: /content/CS540-Statistical-Machine-Learning/Homework 5/self_supervised\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        },
        "id": "Y6nSqw_nrhA-",
        "outputId": "1b479234-9a64-4984-8f7d-1e6a8ce3cd85",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "No such file or directory: 'images/simclr.png'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1299\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1300\u001b[0;31m             \u001b[0mb64_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb2a_base64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1301\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_repr_mimebundle_\u001b[0;34m(self, include, exclude)\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m             \u001b[0mmimetype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mimetype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1290\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_and_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malways_both\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1291\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m                 \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mmimetype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1300\u001b[0m             \u001b[0mb64_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb2a_base64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m             raise FileNotFoundError(\n\u001b[0m\u001b[1;32m   1303\u001b[0m                 \"No such file or directory: '%s'\" % (self.data))\n\u001b[1;32m   1304\u001b[0m         \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or directory: 'images/simclr.png'"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "No such file or directory: 'images/simclr.png'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1299\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1300\u001b[0;31m             \u001b[0mb64_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb2a_base64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1301\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_repr_png_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FMT_PNG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_and_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_repr_jpeg_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1300\u001b[0m             \u001b[0mb64_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb2a_base64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m             raise FileNotFoundError(\n\u001b[0m\u001b[1;32m   1303\u001b[0m                 \"No such file or directory: '%s'\" % (self.data))\n\u001b[1;32m   1304\u001b[0m         \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or directory: 'images/simclr.png'"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Run this cell to view the SimCLR architecture.\n",
        "from IPython.display import Image\n",
        "Image('images/simclr.png', width=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jG3YmI5YrhBA"
      },
      "source": [
        "Given an image **x**, SimCLR uses two different data augmentation schemes **t** and **t'** to generate the positive pair of images **$\\hat{x}_i$** and **$\\hat{x}_j$**. $f$ is a basic encoder net that  extracts representation vectors from the augmented data samples, which yields **$h_i$** and **$h_j$**, respectively. Finally, a small neural network projection head $g$ maps the representation vectors to the space where the contrastive loss is applied. The goal of the contrastive loss is to maximize agreement between the final vectors **$z_i = g(h_i)$** and **$z_j = g(h_j)$**. We will discuss the contrastive loss in more detail later, and you will get to implement it.\n",
        "\n",
        "After training is completed, we throw away the projection head $g$ and only use $f$ and the representation $h$ to perform downstream tasks, such as classification. You will get a chance to finetune a layer on top of a trained SimCLR model for a classification task and compare its performance with a baseline model (without self-supervised learning)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvNBLihfrhBD"
      },
      "source": [
        "## Pretrained Weights\n",
        "For your convenience, we have given you pretrained weights (trained for ~18 hours on CIFAR-10) for the SimCLR model. Run the following cell to download pretrained model weights to be used later. (This will take ~1 minute)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qy6960j4H3QT"
      },
      "outputs": [],
      "source": [
        "# set the path to the pre-trained model in pretrained_simclr_model.pth\n",
        "pretrained_path =\"./pretrained_model/pretrained_simclr_model.pth\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Kssxyfx3mSP"
      },
      "source": [
        "# Data Augmentation\n",
        "\n",
        "Our first step is to perform data augmentation. Implement the `compute_train_transform()` function in `data_utils.py` to apply the following random transformations:\n",
        "\n",
        "1. Randomly resize and crop to 32x32.\n",
        "2. Horizontally flip the image with probability 0.5\n",
        "3. With a probability of 0.8, apply color jitter (see `compute_train_transform()` for definition)\n",
        "4. With a probability of 0.2, convert the image to grayscale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzNT5SvZrhBL"
      },
      "source": [
        "Now complete `compute_train_transform()` and `CIFAR10Pair.__getitem__()` in `data_utils.py` to apply the data augmentation transform and generate **$\\hat{x}_i$** and **$\\hat{x}_j$**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fx_VMEeT3mSP"
      },
      "source": [
        "Test to make sure that your data augmentation code is correct:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cBFOd_A3mSQ",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from data_utils import *\n",
        "from contrastive_loss import *\n",
        "\n",
        "answers = torch.load('simclr_sanity_check.key')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkNoIOy3rhBP"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "def test_data_augmentation(correct_output=None):\n",
        "    train_transform = compute_train_transform(seed=2147483647)\n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=2, shuffle=False, num_workers=2)\n",
        "    dataiter = iter(trainloader)\n",
        "    images, labels = next(dataiter)\n",
        "    img = torchvision.utils.make_grid(images)\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "    output = images\n",
        "\n",
        "    print(\"Maximum error in data augmentation: %g\"%rel_error( output.numpy(), correct_output.numpy()))\n",
        "\n",
        "# Should be less than 1e-07.\n",
        "test_data_augmentation(answers['data_augmentation'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i76TPfFL3mSQ"
      },
      "source": [
        "# Base Encoder and Projection Head\n",
        "The next steps are to apply the base encoder and projection head to the augmented samples **$\\hat{x}_i$** and **$\\hat{x}_j$**.\n",
        "\n",
        "The base encoder $f$ extracts representation vectors for the augmented samples. The SimCLR paper found that using deeper and wider models improved performance and thus chose [ResNet](https://arxiv.org/pdf/1512.03385.pdf) to use as the base encoder. The output of the base encoder are the representation vectors **$h_i = f(\\hat{x}_i$)** and **$h_j = f(\\hat{x}_j$)**.\n",
        "\n",
        "The projection head $g$ is a small neural network that maps the representation vectors **$h_i$** and **$h_j$** to the space where the contrastive loss is applied. The paper found that using a nonlinear projection head improved the representation quality of the layer before it. Specifically, they used a MLP with one hidden layer as the projection head $g$. The contrastive loss is then computed based on the outputs **$z_i = g(h_i$)** and **$z_j = g(h_j$)**.\n",
        "\n",
        "We provide implementations of these two parts in `model.py`. Please skim through the file and make sure you understand the implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_9ORIPN3mSQ"
      },
      "source": [
        "# SimCLR: Contrastive Loss\n",
        "\n",
        "A mini-batch of $N$ training images yields a total of $2N$ data-augmented examples. For each positive pair $(i, j)$ of augmented examples, the contrastive loss function aims to maximize the agreement of vectors $z_i$ and $z_j$. Specifically, the loss is the normalized temperature-scaled cross entropy loss and aims to maximize the agreement of $z_i$ and $z_j$ relative to all other augmented examples in the batch:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q95dPqG3rhBS"
      },
      "source": [
        "$$\n",
        "l \\; (i, j) = -\\log \\frac{\\exp (\\;\\text{sim}(z_i, z_j)\\; / \\;\\tau) }{\\sum_{k=1}^{2N} \\mathbb{1}_{k \\neq i} \\exp (\\;\\text{sim} (z_i, z_k) \\;/ \\;\\tau) }\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNO6WY3vrhBS"
      },
      "source": [
        "where $\\mathbb{1} \\in \\{0, 1\\}$ is an indicator function that outputs $1$ if $k\\neq i$ and $0$ otherwise. $\\tau$ is a temperature parameter that determines how fast the exponentials increase.\n",
        "\n",
        "sim$(z_i, z_j) = \\frac{z_i \\cdot z_j}{|| z_i || || z_j ||}$ is the (normalized) dot product between vectors $z_i$ and $z_j$. The higher the similarity between $z_i$ and $z_j$, the larger the dot product is, and the larger the numerator becomes. The denominator normalizes the value by summing across $z_i$ and all other augmented examples $k$ in the batch. The range of the normalized value is $(0, 1)$, where a high score close to $1$ corresponds to a high similarity between the positive pair $(i, j)$ and low similarity between $i$ and other augmented examples $k$ in the batch. The negative log then maps the range $(0, 1)$ to the loss values $(\\inf, 0)$.\n",
        "\n",
        "The total loss is computed across all positive pairs $(i, j)$ in the batch. Let $z = [z_1, z_2, ..., z_{2N}]$ include all the augmented examples in the batch, where $z_{1}...z_{N}$ are outputs of the left branch, and $z_{N+1}...z_{2N}$ are outputs of the right branch. Thus, the positive pairs are $(z_{k}, z_{k + N})$ for $\\forall k \\in [1, N]$.\n",
        "\n",
        "Then, the total loss $L$ is:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THBrdnCSrhBT"
      },
      "source": [
        "$$\n",
        "L = \\frac{1}{2N} \\sum_{k=1}^N [ \\; l(k, \\;k+N) + l(k+N, \\;k)\\;]\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9g5nT1ylrhBU"
      },
      "source": [
        "**NOTE:** this equation is slightly different from the one in the paper. We've rearranged the ordering of the positive pairs in the batch, so the indices are different. The rearrangement makes it easier to implement the code in vectorized form.\n",
        "\n",
        "We'll walk through the steps of implementing the loss function in vectorized form. Implement the functions `sim`, `simclr_loss_naive` in `contrastive_loss.py`. Test your code by running the sanity checks below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-CiVKoe3mSR"
      },
      "outputs": [],
      "source": [
        "from contrastive_loss import *\n",
        "answers = torch.load('simclr_sanity_check.key')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VMpPjXc3mSS"
      },
      "outputs": [],
      "source": [
        "def test_sim(left_vec, right_vec, correct_output):\n",
        "    output = sim(left_vec, right_vec).cpu().numpy()\n",
        "    print(\"Maximum error in sim: %g\"%rel_error(correct_output.numpy(), output))\n",
        "\n",
        "# Should be less than 1e-07.\n",
        "test_sim(answers['left'][0], answers['right'][0], answers['sim'][0])\n",
        "test_sim(answers['left'][1], answers['right'][1], answers['sim'][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMpreOIw3mSS"
      },
      "outputs": [],
      "source": [
        "def test_loss_naive(left, right, tau, correct_output):\n",
        "    naive_loss = simclr_loss_naive(left, right, tau).item()\n",
        "    print(\"Maximum error in simclr_loss_naive: %g\"%rel_error(correct_output, naive_loss))\n",
        "\n",
        "# Should be less than 1e-07.\n",
        "test_loss_naive(answers['left'], answers['right'], 5.0, answers['loss']['5.0'])\n",
        "test_loss_naive(answers['left'], answers['right'], 1.0, answers['loss']['1.0'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cEoU6hM3mSS"
      },
      "source": [
        "Now implement the vectorized version by implementing `sim_positive_pairs`, `compute_sim_matrix`, `simclr_loss_vectorized` in `contrastive_loss.py`.  Test your code by running the sanity checks below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uW4g-aB_3mSS"
      },
      "outputs": [],
      "source": [
        "def test_sim_positive_pairs(left, right, correct_output):\n",
        "    sim_pair = sim_positive_pairs(left, right).cpu().numpy()\n",
        "    print(\"Maximum error in sim_positive_pairs: %g\"%rel_error(correct_output.numpy(), sim_pair))\n",
        "\n",
        "# Should be less than 1e-07.\n",
        "test_sim_positive_pairs(answers['left'], answers['right'], answers['sim'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fm5z3N7k3mST"
      },
      "outputs": [],
      "source": [
        "def test_sim_matrix(left, right, correct_output):\n",
        "    out = torch.cat([left, right], dim=0)\n",
        "    sim_matrix = compute_sim_matrix(out).cpu()\n",
        "    assert torch.isclose(sim_matrix, correct_output).all(), \"correct: {}. got: {}\".format(correct_output, sim_matrix)\n",
        "    print(\"Test passed!\")\n",
        "\n",
        "test_sim_matrix(answers['left'], answers['right'], answers['sim_matrix'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5Eps9Pc3mST"
      },
      "outputs": [],
      "source": [
        "def test_loss_vectorized(left, right, tau, correct_output):\n",
        "    vec_loss = simclr_loss_vectorized(left, right, tau, device).item()\n",
        "    print(\"Maximum error in loss_vectorized: %g\"%rel_error(correct_output, vec_loss))\n",
        "\n",
        "# Should be less than 1e-07.\n",
        "test_loss_vectorized(answers['left'], answers['right'], 5.0, answers['loss']['5.0'])\n",
        "test_loss_vectorized(answers['left'], answers['right'], 1.0, answers['loss']['1.0'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcSOdwYsrhBb"
      },
      "source": [
        "# Implement the train function\n",
        "Complete the `train()` function in `utils.py` to obtain the model's output and use `simclr_loss_vectorized` to compute the loss. (Please take a look at the `Model` class in `model.py` to understand the model pipeline and the returned values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEutTUosrhBb"
      },
      "outputs": [],
      "source": [
        "from data_utils import *\n",
        "from model import *\n",
        "from utils import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCXOIHqWrhBb"
      },
      "source": [
        "### Train the SimCLR model\n",
        "\n",
        "Run the following cells to load in the pretrained weights and continue to train a little bit more. This part will take ~10 minutes and will output to `pretrained_model/trained_simclr_model.pth`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHXYbTba3mSU"
      },
      "outputs": [],
      "source": [
        "# Do not modify this cell.\n",
        "feature_dim = 128\n",
        "temperature = 0.5\n",
        "k = 200\n",
        "batch_size = 64\n",
        "epochs = 1\n",
        "temperature = 0.5\n",
        "percentage = 0.5\n",
        "pretrained_path = './pretrained_model/pretrained_simclr_model.pth'\n",
        "\n",
        "# Prepare the data.\n",
        "train_transform = compute_train_transform()\n",
        "train_data = CIFAR10Pair(root='data', train=True, transform=train_transform, download=True)\n",
        "train_data = torch.utils.data.Subset(train_data, list(np.arange(int(len(train_data)*percentage))))\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True, drop_last=True)\n",
        "test_transform = compute_test_transform()\n",
        "memory_data = CIFAR10Pair(root='data', train=True, transform=test_transform, download=True)\n",
        "memory_loader = DataLoader(memory_data, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True)\n",
        "test_data = CIFAR10Pair(root='data', train=False, transform=test_transform, download=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True)\n",
        "\n",
        "# Set up the model and optimizer config.\n",
        "model = Model(feature_dim)\n",
        "model.load_state_dict(torch.load(pretrained_path, map_location=device), strict=False)\n",
        "model = model.to(device)\n",
        "flops, params = profile(model, inputs=(torch.randn(1, 3, 32, 32).to(device),))\n",
        "flops, params = clever_format([flops, params])\n",
        "print('# Model Params: {} FLOPs: {}'.format(params, flops))\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)\n",
        "c = len(memory_data.classes)\n",
        "\n",
        "# Training loop.\n",
        "results = {'train_loss': [], 'test_acc@1': [], 'test_acc@5': []} #<< -- output\n",
        "\n",
        "if not os.path.exists('results'):\n",
        "    os.mkdir('results')\n",
        "best_acc = 0.0\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss = train(model, train_loader, optimizer, epoch, epochs, batch_size=batch_size, temperature=temperature, device=device)\n",
        "    results['train_loss'].append(train_loss)\n",
        "    test_acc_1, test_acc_5 = test(model, memory_loader, test_loader, epoch, epochs, c, k=k, temperature=temperature, device=device)\n",
        "    results['test_acc@1'].append(test_acc_1)\n",
        "    results['test_acc@5'].append(test_acc_5)\n",
        "\n",
        "    # Save statistics.\n",
        "    if test_acc_1 > best_acc:\n",
        "        best_acc = test_acc_1\n",
        "        torch.save(model.state_dict(), './pretrained_model/trained_simclr_model.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bm8P70VD3mST"
      },
      "source": [
        "# Finetune a Linear Layer for Classification!\n",
        "\n",
        "Now it's time to put the representation vectors to the test!\n",
        "\n",
        "We remove the projection head from the SimCLR model and slap on a linear layer to finetune for a simple classification task. All layers before the linear layer are frozen, and only the weights in the final linear layer are trained. We compare the performance of the SimCLR + finetuning model against a baseline model, where no self-supervised learning is done beforehand, and all weights in the model are trained. You will get to see for yourself the power of self-supervised learning and how the learned representation vectors improve downstream task performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBCNpj37rhBd"
      },
      "source": [
        "## Baseline: Without Self-Supervised Learning\n",
        "First, let's take a look at the baseline model. We'll remove the projection head from the SimCLR model and slap on a linear layer to finetune for a simple classification task. No self-supervised learning is done beforehand, and all weights in the model are trained. Run the following cells.\n",
        "\n",
        "**NOTE:** Don't worry if you see low but reasonable performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0fOWHzXrhBd",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, num_class):\n",
        "        super(Classifier, self).__init__()\n",
        "\n",
        "        # Encoder.\n",
        "        self.f = Model().f\n",
        "\n",
        "        # Classifier.\n",
        "        self.fc = nn.Linear(2048, num_class, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.f(x)\n",
        "        feature = torch.flatten(x, start_dim=1)\n",
        "        out = self.fc(feature)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uqsLItcrhBe",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Do not modify this cell.\n",
        "feature_dim = 128\n",
        "temperature = 0.5\n",
        "k = 200\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "percentage = 0.1\n",
        "\n",
        "\n",
        "\n",
        "train_transform = compute_train_transform()\n",
        "train_data = CIFAR10(root='data', train=True, transform=train_transform, download=True)\n",
        "trainset = torch.utils.data.Subset(train_data, list(np.arange(int(len(train_data)*percentage))))\n",
        "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True)\n",
        "test_transform = compute_test_transform()\n",
        "test_data = CIFAR10(root='data', train=False, transform=test_transform, download=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True)\n",
        "\n",
        "model = Classifier(num_class=len(train_data.classes)).to(device)\n",
        "for param in model.f.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "flops, params = profile(model, inputs=(torch.randn(1, 3, 32, 32).to(device),))\n",
        "flops, params = clever_format([flops, params])\n",
        "print('# Model Params: {} FLOPs: {}'.format(params, flops))\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=1e-3, weight_decay=1e-6)\n",
        "no_pretrain_results = {'train_loss': [], 'train_acc@1': [], 'train_acc@5': [],\n",
        "           'test_loss': [], 'test_acc@1': [], 'test_acc@5': []}\n",
        "\n",
        "best_acc = 0.0\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss, train_acc_1, train_acc_5 = train_val(model, train_loader, optimizer, epoch, epochs, device=device)\n",
        "    no_pretrain_results['train_loss'].append(train_loss)\n",
        "    no_pretrain_results['train_acc@1'].append(train_acc_1)\n",
        "    no_pretrain_results['train_acc@5'].append(train_acc_5)\n",
        "    test_loss, test_acc_1, test_acc_5 = train_val(model, test_loader, None, epoch, epochs,device=device)\n",
        "    no_pretrain_results['test_loss'].append(test_loss)\n",
        "    no_pretrain_results['test_acc@1'].append(test_acc_1)\n",
        "    no_pretrain_results['test_acc@5'].append(test_acc_5)\n",
        "    if test_acc_1 > best_acc:\n",
        "        best_acc = test_acc_1\n",
        "\n",
        "# Print the best test accuracy.\n",
        "print('Best top-1 accuracy without self-supervised learning: ', best_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXiFpkEhrhBe"
      },
      "source": [
        "## With Self-Supervised Learning\n",
        "\n",
        "Let's see how much improvement we get with self-supervised learning. Here, we pretrain the SimCLR model using the simclr loss you wrote, remove the projection head from the SimCLR model, and use a linear layer to finetune for a simple classification task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssl_best_accuracy"
      },
      "outputs": [],
      "source": [
        "# Do not modify this cell.\n",
        "feature_dim = 128\n",
        "temperature = 0.5\n",
        "k = 200\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "percentage = 0.1\n",
        "pretrained_path = './pretrained_model/trained_simclr_model.pth'\n",
        "\n",
        "train_transform = compute_train_transform()\n",
        "train_data = CIFAR10(root='data', train=True, transform=train_transform, download=True)\n",
        "trainset = torch.utils.data.Subset(train_data, list(np.arange(int(len(train_data)*percentage))))\n",
        "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True)\n",
        "test_transform = compute_test_transform()\n",
        "test_data = CIFAR10(root='data', train=False, transform=test_transform, download=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True)\n",
        "\n",
        "model = Classifier(num_class=len(train_data.classes))\n",
        "model.load_state_dict(torch.load(pretrained_path, map_location=device), strict=False)\n",
        "model = model.to(device)\n",
        "for param in model.f.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "flops, params = profile(model, inputs=(torch.randn(1, 3, 32, 32).to(device),))\n",
        "flops, params = clever_format([flops, params])\n",
        "print('# Model Params: {} FLOPs: {}'.format(params, flops))\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=1e-3, weight_decay=1e-6)\n",
        "pretrain_results = {'train_loss': [], 'train_acc@1': [], 'train_acc@5': [],\n",
        "           'test_loss': [], 'test_acc@1': [], 'test_acc@5': []}\n",
        "\n",
        "best_acc = 0.0\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss, train_acc_1, train_acc_5 = train_val(model, train_loader, optimizer, epoch, epochs,device=device)\n",
        "    pretrain_results['train_loss'].append(train_loss)\n",
        "    pretrain_results['train_acc@1'].append(train_acc_1)\n",
        "    pretrain_results['train_acc@5'].append(train_acc_5)\n",
        "    test_loss, test_acc_1, test_acc_5 = train_val(model, test_loader, None, epoch, epochs,device=device)\n",
        "    pretrain_results['test_loss'].append(test_loss)\n",
        "    pretrain_results['test_acc@1'].append(test_acc_1)\n",
        "    pretrain_results['test_acc@5'].append(test_acc_5)\n",
        "    if test_acc_1 > best_acc:\n",
        "        best_acc = test_acc_1\n",
        "\n",
        "# Print the best test accuracy. You should see a best top-1 accuracy of >=70%.\n",
        "print('Best top-1 accuracy with self-supervised learning: ', best_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnkxQJPnrhBf"
      },
      "source": [
        "### Plot your Comparison\n",
        "\n",
        "Plot the test accuracies between the baseline model (no pretraining) and same model pretrained with self-supervised learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHKw8mlWrhBg"
      },
      "outputs": [],
      "source": [
        "plt.plot(no_pretrain_results['test_acc@1'], label=\"Without Pretrain\")\n",
        "plt.plot(pretrain_results['test_acc@1'], label=\"With Pretrain\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Test Top-1 Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKYErbFYH3QZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Self_Supervised_Learning.ipynb",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}