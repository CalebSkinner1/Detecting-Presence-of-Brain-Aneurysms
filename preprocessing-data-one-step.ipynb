{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":99552,"databundleVersionId":13851420,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nimport os, json, pathlib, shutil\nimport gc\nimport re\nimport cv2\nimport math\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport pydicom\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision.models as models\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, TensorDataset, DataLoader, random_split, Subset\nimport timm\nfrom collections import defaultdict\nfrom typing import List, Tuple\nimport shutil\nimport matplotlib.pyplot as plt\nimport random\nimport sklearn\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\nfrom IPython.display import display\nimport joblib\nfrom joblib import Parallel, delayed\nfrom pathlib import Path\nimport h5py, numpy as np\n\nROOT = \"/kaggle/input/rsna-intracranial-aneurysm-detection\"  \nsys.path.append(ROOT)  # parent of kaggle_evaluation\n\nimport kaggle_evaluation.rsna_inference_server as rsna_eval","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def _unit(v):\n    v = np.asarray(v, float)\n    n = np.linalg.norm(v)\n    return v / (n + 1e-12)\n\ndef _slice_normal_from_iop(iop):\n    r = _unit(iop[:3]) # row direction\n    c = _unit(iop[3:]) # col direction\n    n = np.cross(r, c) # slice normal\n    return _unit(n)\n\ndef _scalar_pos_along_normal(ds, n):\n    # project IPP onto the normal (dot product) (https://discovery.ucl.ac.uk/id/eprint/10146893/1/geometry_medim.pdf)\n    p = np.asarray(getattr(ds, \"ImagePositionPatient\", [0,0,0]), float) # or default to [0,0,0]\n    return float(np.dot(n, p))\n\ndef _physical_sorted_paths(files):\n    # read first file to get IOP (row/col vectors)\n    ds0 = pydicom.dcmread(str(files[0]), stop_before_pixels=True) # assuming IOP is the same for all slices\n    iop = np.asarray(getattr(ds0, \"ImageOrientationPatient\", [1,0,0,0,1,0]), float) # or default to [1,0,0,0,1,0]\n    n = _slice_normal_from_iop(iop)\n\n    keyed = []\n    for fp in files:\n        ds = pydicom.dcmread(str(fp), stop_before_pixels=True)\n        s = _scalar_pos_along_normal(ds, n)\n        keyed.append((s, fp)) # pair of tuples (call this pair t)\n    keyed.sort(key=lambda t: t[0]) # sort by first thing in pair (s)    \n    return [pair[1] for pair in keyed], n # once sorted, only need file\n\ndef _percentile_slice_indices(Z, target_slices, p_lo=15.0, p_hi=85.0):\n    if Z <= 0 or target_slices <= 0:\n        return np.array([], dtype=int)\n            \n    # percentiles spaced linearly from p_lo to p_hi (inclusive)\n    ps = np.linspace(p_lo, p_hi, num=target_slices)\n    idx = np.round((ps / 100.0) * (Z - 1)).astype(int)\n    idx = np.clip(idx, 0, Z - 1)\n\n    # de-duplicate while preserving order (can happen if Z is small) \n    uniq, first_pos = np.unique(idx, return_index=True) \n    idx = idx[np.sort(first_pos)] # Same values as np.unique, but derived from original order\n    # pad if needed\n    if len(idx) < target_slices:\n        pad = np.full(target_slices - len(idx), idx[-1] if len(idx) else 0, dtype=int)\n        idx = np.concatenate([idx, pad])\n    return idx","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_series(series_dir, target_hw = 256, target_slices = 32, \n                      p_lo = 15.0, p_hi = 85.0):\n    series_dir = Path(series_dir)\n    files = sorted(series_dir.glob(\"*.dcm\"))\n    if not files:\n        raise FileNotFoundError(f\"No DICOMs in {series_dir}\")\n\n    # slice ordering\n    try:\n        paths, n = _physical_sorted_paths(files)\n    except Exception:\n        def sort_key(fp):\n            ds = pydicom.dcmread(str(fp), stop_before_pixels=True)\n            inst = getattr(ds, \"InstanceNumber\", 0)\n            return float(inst)\n        paths = sorted(files, key=sort_key)\n\n    Z = len(paths)\n    if Z == 0:\n        return np.empty((0, target_hw, target_hw), dtype=np.float32)\n\n    # choose which indices to load\n    if target_slices is None:\n        # use ALL slices\n        sel_paths = paths\n    else:\n        # percentile subset\n        idx = _percentile_slice_indices(Z, target_slices, p_lo=p_lo, p_hi=p_hi)\n        sel_paths = [paths[i] for i in idx]\n\n    # read one header for modality\n    ds0 = pydicom.dcmread(str(paths[0]), stop_before_pixels=True)\n    modality = getattr(ds0, \"Modality\", \"Unknown\")\n\n    # preallocate final volume \n    vol = np.empty((len(sel_paths), target_hw, target_hw), dtype=np.float32)\n\n    for k, fp in enumerate(sel_paths):\n        ds = pydicom.dcmread(str(fp), force=True, defer_size=\"1 KB\")\n\n        arr = ds.pixel_array  \n        if arr.ndim == 3 and arr.shape[0] > 1 and arr.shape[-1] != 3:\n            arr = arr.mean(axis=0).astype(arr.dtype, copy=False)\n        if arr.ndim == 3 and arr.shape[-1] == 3:  # RGB -> gray\n            arr = arr.mean(axis=-1)\n\n        arr_small = cv2.resize(arr, (target_hw, target_hw), interpolation=cv2.INTER_AREA)\n\n        img = arr_small.astype(np.float32, copy=False)\n        slope = float(getattr(ds, \"RescaleSlope\", 1.0))\n        inter = float(getattr(ds, \"RescaleIntercept\", 0.0))\n        img *= slope\n        img += inter\n\n        if getattr(ds, \"PhotometricInterpretation\", \"MONOCHROME2\") == \"MONOCHROME1\":\n            mmax = float(img.max()); mmin = float(img.min())\n            img *= -1.0\n            img += (mmax + mmin)\n\n        vol[k] = img\n\n        del ds, arr, arr_small, img\n\n    # clip / normalize (in-place)  \n    if modality in {\"CT\", \"CTA\"}:\n        lo, hi = 0.0, 500.0   \n    else:\n        vals = vol[vol != 0]\n        if vals.size >= 100:\n            lo, hi = np.percentile(vals, [5, 95])\n        else:\n            lo, hi = np.percentile(vol, [5, 95])\n\n    np.clip(vol, lo, hi, out=vol)\n    vol -= lo\n    vol /= (hi - lo + 1e-6)\n\n    return vol.astype(np.float16, copy=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ID_COL = 'SeriesInstanceUID'\n\n# 13 location columns\nLOCATION_COLS = [\n    'Left Infraclinoid Internal Carotid Artery',\n    'Right Infraclinoid Internal Carotid Artery',\n    'Left Supraclinoid Internal Carotid Artery',\n    'Right Supraclinoid Internal Carotid Artery',\n    'Left Middle Cerebral Artery',\n    'Right Middle Cerebral Artery',\n    'Anterior Communicating Artery',\n    'Left Anterior Cerebral Artery',\n    'Right Anterior Cerebral Artery',\n    'Left Posterior Communicating Artery',\n    'Right Posterior Communicating Artery',\n    'Basilar Tip',\n    'Other Posterior Circulation',\n]\n\nLABEL_COLS = LOCATION_COLS + ['Aneurysm Present']\n\ndata_path = Path(\"/kaggle/input/rsna-intracranial-aneurysm-detection\")\nseries_dir = data_path / \"series\"\n\n# read labels\ndf = pd.read_csv(data_path / \"train.csv\")\n\n# add the folder path for each series\ndf['path'] = df['SeriesInstanceUID'].apply(lambda uid: series_dir / str(uid))\n\n# keep only rows whose folder actually exists\ndf = df[df['path'].apply(lambda p: p.exists())].reset_index(drop = True)\n\n# take a balanced sample\nn_per_class = 1863\nif n_per_class is not None:\n    pos = df[df[\"Aneurysm Present\"] == 1].sample(min(n_per_class, (df[\"Aneurysm Present\"] == 1).sum()), random_state = 0)\n    neg = df[df[\"Aneurysm Present\"] == 0].sample(min(n_per_class, (df[\"Aneurysm Present\"] == 0).sum()), random_state = 0)\n    df = pd.concat([pos, neg]).sample(frac = 1, random_state = 0).reset_index(drop = True)\n\n# ensure labels are numerical ints\ndf[LABEL_COLS] = (\n    df[LABEL_COLS].apply(pd.to_numeric, errors = 'coerce').fillna(0).astype(int)\n)\n\nseries_list = list(zip(df['path'].tolist(), df[LABEL_COLS].to_numpy(dtype=int)))\n\nprint(len(series_list), \"series ready\")\nprint(series_list[0][0]) # a path\nprint(series_list[0][1]) # a label","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Canonical order so all shards pick the SAME subjects deterministically\nseries_list = sorted(series_list, key=lambda t: t[0].name) # t = (Path, labels)\nN = len(series_list)\nprint(\"Total subjects:\", N)\n\nK = 4  # total shards\nshard_id = 1 # CHANGE per notebook\n\n# which subjects belong to this shard\nidxs = [i for i in range(N) if i % K == shard_id]\nprint(f\"Shard {shard_id}/{K}: {len(idxs)} subjects\")\n\nH, W = 256, 256\nL = len(LABEL_COLS)\n\nh5_path = f\"/kaggle/working/dataset_shard_{shard_id}_all_slices.h5\"\n\nwith h5py.File(h5_path, \"w\") as f:\n    x_grp = f.create_group(\"x\")\n    y_ds = f.create_dataset(\"y\", shape=(len(idxs), L), dtype=\"int16\", compression=\"gzip\")\n    uid_ds = f.create_dataset(\"uid\", shape=(len(idxs),), dtype=h5py.string_dtype(encoding=\"utf-8\"))\n    zlen_ds = f.create_dataset(\"z_len\", shape=(len(idxs),), dtype=\"int16\")\n\n    for j, i in enumerate(idxs):\n        sd, y = series_list[i]\n\n        vol = preprocess_series(sd, target_hw=256, target_slices= 32) \n        Z_i = vol.shape[0]\n\n        x_grp.create_dataset(\n            name = str(j),\n            data = vol,\n            dtype = \"float16\",\n            compression = \"gzip\"\n        )\n\n        y_ds[j] = y.astype(\"int16\")\n        uid_ds[j] = sd.name\n        zlen_ds[j] = Z_i\n\n        del vol\n        if (j + 1) % 10 == 0:\n            gc.collect()\n\n        if (j + 1) % 50 == 0:\n            f.flush()\n            print(f\"Shard {shard_id}: wrote {j+1}/{len(idxs)}\")\n\nprint(\"Saved shard to:\", h5_path)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-04T23:23:39.724Z"}},"outputs":[],"execution_count":null}]}