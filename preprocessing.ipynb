{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paBPcM_OV9zZ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pydicom\n",
        "from pathlib import Path\n",
        "import h5py, numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMJRwqSwV9za",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Change file path here:\n",
        "ROOT = \"\"\n",
        "\n",
        "ID_COL = 'SeriesInstanceUID'\n",
        "\n",
        "LABEL_COLS = [\n",
        "    'Left Infraclinoid Internal Carotid Artery',\n",
        "    'Right Infraclinoid Internal Carotid Artery',\n",
        "    'Left Supraclinoid Internal Carotid Artery',\n",
        "    'Right Supraclinoid Internal Carotid Artery',\n",
        "    'Left Middle Cerebral Artery',\n",
        "    'Right Middle Cerebral Artery',\n",
        "    'Anterior Communicating Artery',\n",
        "    'Left Anterior Cerebral Artery',\n",
        "    'Right Anterior Cerebral Artery',\n",
        "    'Left Posterior Communicating Artery',\n",
        "    'Right Posterior Communicating Artery',\n",
        "    'Basilar Tip',\n",
        "    'Other Posterior Circulation',\n",
        "    'Aneurysm Present',\n",
        "]\n",
        "\n",
        "# per-slice label order (0..12)\n",
        "SLICE_LABEL_NAMES = [\n",
        "    \"Other Posterior Circulation\",\n",
        "    \"Basilar Tip\",\n",
        "    \"Right Posterior Communicating Artery\",\n",
        "    \"Left Posterior Communicating Artery\",\n",
        "    \"Right Infraclinoid Internal Carotid Artery\",\n",
        "    \"Left Infraclinoid Internal Carotid Artery\",\n",
        "    \"Right Supraclinoid Internal Carotid Artery\",\n",
        "    \"Left Supraclinoid Internal Carotid Artery\",\n",
        "    \"Right Middle Cerebral Artery\",\n",
        "    \"Left Middle Cerebral Artery\",\n",
        "    \"Right Anterior Cerebral Artery\",\n",
        "    \"Left Anterior Cerebral Artery\",\n",
        "    \"Anterior Communicating Artery\",\n",
        "]\n",
        "\n",
        "# text -> index 0..12 for per-slice labels\n",
        "SLICE_LOCATION_TO_IDX = {name: i for i, name in enumerate(SLICE_LABEL_NAMES)}\n",
        "\n",
        "# load train_localizers.csv and group by SeriesInstanceUID\n",
        "loc_df = pd.read_csv(f\"{ROOT}/train_localizers.csv\")\n",
        "loc_by_series = {k: g for k, g in loc_df.groupby(\"SeriesInstanceUID\")}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbeIbHULV9zb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def _unit(v):\n",
        "    v = np.asarray(v, float)\n",
        "    n = np.linalg.norm(v)\n",
        "    return v / (n + 1e-12)\n",
        "\n",
        "def _slice_normal_from_iop(iop):\n",
        "    r = _unit(iop[:3]) # row direction\n",
        "    c = _unit(iop[3:]) # col direction\n",
        "    n = np.cross(r, c) # slice normal\n",
        "    return _unit(n)\n",
        "\n",
        "def _scalar_pos_along_normal(ds, n):\n",
        "    # project IPP onto the normal (dot product) (https://discovery.ucl.ac.uk/id/eprint/10146893/1/geometry_medim.pdf)\n",
        "    p = np.asarray(getattr(ds, \"ImagePositionPatient\", [0,0,0]), float) # or default to [0,0,0]\n",
        "    return float(np.dot(n, p))\n",
        "\n",
        "def _physical_sorted_paths(files):\n",
        "    # read first file to get IOP (row/col vectors)\n",
        "    ds0 = pydicom.dcmread(str(files[0]), stop_before_pixels=True) # assuming IOP is the same for all slices\n",
        "    iop = np.asarray(getattr(ds0, \"ImageOrientationPatient\", [1,0,0,0,1,0]), float) # or default to [1,0,0,0,1,0]\n",
        "    n = _slice_normal_from_iop(iop)\n",
        "\n",
        "    keyed = []\n",
        "    for fp in files:\n",
        "        ds = pydicom.dcmread(str(fp), stop_before_pixels=True)\n",
        "        s = _scalar_pos_along_normal(ds, n)\n",
        "        keyed.append((s, fp)) # pair of tuples (call this pair t)\n",
        "    keyed.sort(key=lambda t: t[0]) # sort by first thing in pair (s)\n",
        "    return [pair[1] for pair in keyed], n # once sorted, only need file\n",
        "\n",
        "def _percentile_slice_indices(Z, target_slices, p_lo=15.0, p_hi=85.0):\n",
        "    if Z <= 0 or target_slices <= 0:\n",
        "        return np.array([], dtype=int)\n",
        "\n",
        "    # percentiles spaced linearly from p_lo to p_hi (inclusive)\n",
        "    ps = np.linspace(p_lo, p_hi, num=target_slices)\n",
        "    idx = np.round((ps / 100.0) * (Z - 1)).astype(int)\n",
        "    idx = np.clip(idx, 0, Z - 1)\n",
        "\n",
        "    # de-duplicate while preserving order (can happen if Z is small)\n",
        "    uniq, first_pos = np.unique(idx, return_index=True)\n",
        "    idx = idx[np.sort(first_pos)] # Same values as np.unique, but derived from original order\n",
        "    # pad if needed\n",
        "    if len(idx) < target_slices:\n",
        "        pad = np.full(target_slices - len(idx), idx[-1] if len(idx) else 0, dtype=int)\n",
        "        idx = np.concatenate([idx, pad])\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgQZP1vJV9zb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def preprocess_series(series_dir, target_hw = 256):\n",
        "    series_dir = Path(series_dir)\n",
        "    files = sorted(series_dir.glob(\"*.dcm\"))\n",
        "    if not files:\n",
        "        raise FileNotFoundError(f\"No DICOMs in {series_dir}\")\n",
        "\n",
        "    # slice ordering\n",
        "    try:\n",
        "        paths, n = _physical_sorted_paths(files)\n",
        "    except Exception:\n",
        "        def sort_key(fp):\n",
        "            ds = pydicom.dcmread(str(fp), stop_before_pixels=True)\n",
        "            return float(getattr(ds, \"InstanceNumber\", 0))\n",
        "        paths = sorted(files, key=sort_key)\n",
        "\n",
        "    # Read all slices, no subsampling\n",
        "    vol_slices = []\n",
        "    sop_uids = []\n",
        "\n",
        "    for fp in paths:\n",
        "        ds = pydicom.dcmread(str(fp), force=True, defer_size=\"1 KB\")\n",
        "        sop = getattr(ds, \"SOPInstanceUID\", None)\n",
        "        if sop is None:\n",
        "            continue  # skip slices with no SOP\n",
        "        sop_uids.append(sop)\n",
        "\n",
        "        arr = ds.pixel_array\n",
        "        if arr.ndim == 3 and arr.shape[0] > 1 and arr.shape[-1] != 3:\n",
        "            arr = arr.mean(axis=0)\n",
        "        if arr.ndim == 3 and arr.shape[-1] == 3:\n",
        "            arr = arr.mean(axis=-1)\n",
        "\n",
        "        arr_small = cv2.resize(arr, (target_hw, target_hw), interpolation=cv2.INTER_AREA)\n",
        "        img = arr_small.astype(np.float32)\n",
        "        img *= float(getattr(ds, \"RescaleSlope\", 1.0))\n",
        "        img += float(getattr(ds, \"RescaleIntercept\", 0.0))\n",
        "\n",
        "        if getattr(ds, \"PhotometricInterpretation\", \"MONOCHROME2\") == \"MONOCHROME1\":\n",
        "            mmax, mmin = float(img.max()), float(img.min())\n",
        "            img *= -1.0\n",
        "            img += (mmax + mmin)\n",
        "\n",
        "        vol_slices.append(img)\n",
        "\n",
        "    vol = np.stack(vol_slices).astype(np.float32)\n",
        "\n",
        "    modality = getattr(pydicom.dcmread(str(paths[0]), stop_before_pixels=True),\n",
        "                       \"Modality\", \"Unknown\")\n",
        "\n",
        "    if modality in {\"CT\", \"CTA\"}:\n",
        "        # lo, hi = -100.0, 500.0\n",
        "        vals = vol[vol != 0]\n",
        "        lo, hi = np.percentile(vals, [5, 95]) if vals.size >= 100 else np.percentile(vol, [5,95])\n",
        "    else:\n",
        "        vals = vol[vol != 0]\n",
        "        lo, hi = np.percentile(vals, [5, 95]) if vals.size >= 100 else np.percentile(vol, [5,95])\n",
        "\n",
        "    np.clip(vol, lo, hi, out=vol)\n",
        "    vol -= lo\n",
        "    vol /= (hi - lo + 1e-6)\n",
        "\n",
        "    return vol.astype(np.float16), sop_uids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhLipCnvV9zc",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "data_path = Path(f\"{ROOT}\")\n",
        "series_dir = data_path / \"series\"\n",
        "\n",
        "# read labels\n",
        "df = pd.read_csv(data_path / \"train.csv\")\n",
        "\n",
        "# add the folder path for each series\n",
        "df['path'] = df['SeriesInstanceUID'].apply(lambda uid: series_dir / str(uid))\n",
        "\n",
        "# keep only rows whose folder actually exists\n",
        "df = df[df['path'].apply(lambda p: p.exists())].reset_index(drop = True)\n",
        "\n",
        "# take a balanced sample\n",
        "n_per_class = 1863\n",
        "if n_per_class is not None:\n",
        "    pos = df[df[\"Aneurysm Present\"] == 1].sample(min(n_per_class, (df[\"Aneurysm Present\"] == 1).sum()), random_state = 0)\n",
        "    neg = df[df[\"Aneurysm Present\"] == 0].sample(min(n_per_class, (df[\"Aneurysm Present\"] == 0).sum()), random_state = 0)\n",
        "    df = pd.concat([pos, neg]).sample(frac = 1, random_state = 0).reset_index(drop = True)\n",
        "\n",
        "# ensure labels are numerical ints\n",
        "df[LABEL_COLS] = (\n",
        "    df[LABEL_COLS].apply(pd.to_numeric, errors = 'coerce').fillna(0).astype(int)\n",
        ")\n",
        "\n",
        "series_list = list(zip(df['path'].tolist(), df[LABEL_COLS].to_numpy(dtype=int)))\n",
        "\n",
        "print(len(series_list), \"series ready\")\n",
        "print(series_list[0][0]) # a path\n",
        "print(series_list[0][1]) # a label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WB9s-O6ZV9zc",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "series_list = sorted(series_list, key=lambda t: t[0].name) # t = (Path, labels)\n",
        "N = len(series_list)\n",
        "print(\"Total subjects:\", N)\n",
        "\n",
        "K = 4  # total shards\n",
        "shard_id = 0  # CHANGE per notebook\n",
        "\n",
        "# which subjects belong to this shard\n",
        "idxs = [i for i in range(N) if i % K == shard_id]\n",
        "print(f\"Shard {shard_id}/{K}: {len(idxs)} subjects\")\n",
        "\n",
        "H, W = 256, 256\n",
        "L = len(LABEL_COLS)\n",
        "\n",
        "h5_path = f\"{ROOT}/shards/shard_{shard_id}.h5\"\n",
        "\n",
        "with h5py.File(h5_path, \"w\") as f:\n",
        "    x_grp = f.create_group(\"x\")\n",
        "    y_ds = f.create_dataset(\"y\", shape=(len(idxs), L), dtype=\"int16\", compression=\"gzip\")\n",
        "    uid_ds = f.create_dataset(\"uid\", shape=(len(idxs),), dtype=h5py.string_dtype(encoding=\"utf-8\"))\n",
        "    zlen_ds = f.create_dataset(\"z_len\", shape=(len(idxs),), dtype=\"int16\")\n",
        "    yslice_grp = f.create_group(\"y_slice\")\n",
        "\n",
        "    for j, i in enumerate(idxs):\n",
        "        sd, y = series_list[i] # sd is a Path to the series directory\n",
        "        series_uid = sd.name # this matches SeriesInstanceUID in CSVs\n",
        "\n",
        "        vol, sop_uids = preprocess_series(sd, target_hw=256)\n",
        "        Z_i = vol.shape[0]\n",
        "\n",
        "        # build per-slice y_slice (Z_i, 14)\n",
        "        y_slice = np.zeros((Z_i, 14), dtype=\"int16\")\n",
        "\n",
        "        rows = loc_by_series.get(series_uid, None) # all localizers for this series\n",
        "        if rows is not None:\n",
        "            # map SOPInstanceUID -> slice index in vol\n",
        "            uid_to_z = {uid: z for z, uid in enumerate(sop_uids)}\n",
        "\n",
        "            for _, row_loc in rows.iterrows():\n",
        "                sop = row_loc[\"SOPInstanceUID\"]\n",
        "                if sop not in uid_to_z:\n",
        "                    continue\n",
        "\n",
        "                z = uid_to_z[sop] # slice index 0..Z_i-1\n",
        "                loc_name = row_loc[\"location\"]\n",
        "\n",
        "                if loc_name not in SLICE_LOCATION_TO_IDX:\n",
        "                    # unexpected text; skip\n",
        "                    continue\n",
        "\n",
        "                loc_idx = SLICE_LOCATION_TO_IDX[loc_name]  # 0..12\n",
        "\n",
        "                # set the one-hot location and \"aneurysm present on this slice\"\n",
        "                y_slice[z, loc_idx] = 1\n",
        "                y_slice[z, 13]      = 1\n",
        "\n",
        "        # write data to HDF5\n",
        "        x_grp.create_dataset(\n",
        "            name=str(j),\n",
        "            data=vol,\n",
        "            dtype=\"float16\",\n",
        "            compression=\"gzip\"\n",
        "        )\n",
        "\n",
        "        # subject-level y\n",
        "        y_ds[j] = y.astype(\"int16\")\n",
        "        uid_ds[j] = series_uid\n",
        "        zlen_ds[j] = Z_i\n",
        "\n",
        "        # per-slice labels\n",
        "        yslice_grp.create_dataset(\n",
        "            name = str(j),\n",
        "            data = y_slice,\n",
        "            dtype = \"int16\",\n",
        "            compression = \"gzip\"\n",
        "        )\n",
        "\n",
        "        del vol, y_slice\n",
        "        if (j + 1) % 10 == 0:\n",
        "            gc.collect()\n",
        "\n",
        "        if (j + 1) % 50 == 0:\n",
        "            f.flush()\n",
        "            print(f\"Shard {shard_id}: wrote {j+1}/{len(idxs)}\")\n",
        "\n",
        "print(\"Saved shard to:\", h5_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "databundleVersionId": 13851420,
          "sourceId": 99552,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31192,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
