{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T21:57:21.911462Z",
     "iopub.status.busy": "2025-12-04T21:57:21.911116Z",
     "iopub.status.idle": "2025-12-04T21:57:28.597621Z",
     "shell.execute_reply": "2025-12-04T21:57:28.596822Z",
     "shell.execute_reply.started": "2025-12-04T21:57:21.911435Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gc\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "from pathlib import Path\n",
    "import h5py, numpy as np\n",
    "\n",
    "ROOT = \"/kaggle/input/rsna-intracranial-aneurysm-detection\"  \n",
    "sys.path.append(ROOT)  # parent of kaggle_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T21:57:32.214110Z",
     "iopub.status.busy": "2025-12-04T21:57:32.213554Z",
     "iopub.status.idle": "2025-12-04T21:57:32.259172Z",
     "shell.execute_reply": "2025-12-04T21:57:32.258396Z",
     "shell.execute_reply.started": "2025-12-04T21:57:32.214082Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ID_COL = 'SeriesInstanceUID'\n",
    "\n",
    "LABEL_COLS = [\n",
    "    'Left Infraclinoid Internal Carotid Artery',\n",
    "    'Right Infraclinoid Internal Carotid Artery',\n",
    "    'Left Supraclinoid Internal Carotid Artery',\n",
    "    'Right Supraclinoid Internal Carotid Artery',\n",
    "    'Left Middle Cerebral Artery',\n",
    "    'Right Middle Cerebral Artery',\n",
    "    'Anterior Communicating Artery',\n",
    "    'Left Anterior Cerebral Artery',\n",
    "    'Right Anterior Cerebral Artery',\n",
    "    'Left Posterior Communicating Artery',\n",
    "    'Right Posterior Communicating Artery',\n",
    "    'Basilar Tip',\n",
    "    'Other Posterior Circulation',\n",
    "    'Aneurysm Present',\n",
    "]\n",
    "\n",
    "# per-slice label order (0..12) \n",
    "SLICE_LABEL_NAMES = [\n",
    "    \"Other Posterior Circulation\",                    \n",
    "    \"Basilar Tip\",                                    \n",
    "    \"Right Posterior Communicating Artery\",           \n",
    "    \"Left Posterior Communicating Artery\",            \n",
    "    \"Right Infraclinoid Internal Carotid Artery\",     \n",
    "    \"Left Infraclinoid Internal Carotid Artery\",      \n",
    "    \"Right Supraclinoid Internal Carotid Artery\",     \n",
    "    \"Left Supraclinoid Internal Carotid Artery\",      \n",
    "    \"Right Middle Cerebral Artery\",                   \n",
    "    \"Left Middle Cerebral Artery\",                    \n",
    "    \"Right Anterior Cerebral Artery\",                 \n",
    "    \"Left Anterior Cerebral Artery\",                  \n",
    "    \"Anterior Communicating Artery\",                  \n",
    "]\n",
    "\n",
    "# text -> index 0..12 for per-slice labels\n",
    "SLICE_LOCATION_TO_IDX = {name: i for i, name in enumerate(SLICE_LABEL_NAMES)}\n",
    "\n",
    "# load train_localizers.csv and group by SeriesInstanceUID \n",
    "loc_df = pd.read_csv(f\"{ROOT}/train_localizers.csv\")\n",
    "loc_by_series = {k: g for k, g in loc_df.groupby(\"SeriesInstanceUID\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T21:57:40.445014Z",
     "iopub.status.busy": "2025-12-04T21:57:40.443837Z",
     "iopub.status.idle": "2025-12-04T21:57:40.455491Z",
     "shell.execute_reply": "2025-12-04T21:57:40.454615Z",
     "shell.execute_reply.started": "2025-12-04T21:57:40.444978Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def _unit(v):\n",
    "    v = np.asarray(v, float)\n",
    "    n = np.linalg.norm(v)\n",
    "    return v / (n + 1e-12)\n",
    "\n",
    "def _slice_normal_from_iop(iop):\n",
    "    r = _unit(iop[:3]) # row direction\n",
    "    c = _unit(iop[3:]) # col direction\n",
    "    n = np.cross(r, c) # slice normal\n",
    "    return _unit(n)\n",
    "\n",
    "def _scalar_pos_along_normal(ds, n):\n",
    "    # project IPP onto the normal (dot product) (https://discovery.ucl.ac.uk/id/eprint/10146893/1/geometry_medim.pdf)\n",
    "    p = np.asarray(getattr(ds, \"ImagePositionPatient\", [0,0,0]), float) # or default to [0,0,0]\n",
    "    return float(np.dot(n, p))\n",
    "\n",
    "def _physical_sorted_paths(files):\n",
    "    # read first file to get IOP (row/col vectors)\n",
    "    ds0 = pydicom.dcmread(str(files[0]), stop_before_pixels=True) # assuming IOP is the same for all slices\n",
    "    iop = np.asarray(getattr(ds0, \"ImageOrientationPatient\", [1,0,0,0,1,0]), float) # or default to [1,0,0,0,1,0]\n",
    "    n = _slice_normal_from_iop(iop)\n",
    "\n",
    "    keyed = []\n",
    "    for fp in files:\n",
    "        ds = pydicom.dcmread(str(fp), stop_before_pixels=True)\n",
    "        s = _scalar_pos_along_normal(ds, n)\n",
    "        keyed.append((s, fp)) # pair of tuples (call this pair t)\n",
    "    keyed.sort(key=lambda t: t[0]) # sort by first thing in pair (s)    \n",
    "    return [pair[1] for pair in keyed], n # once sorted, only need file\n",
    "\n",
    "def _percentile_slice_indices(Z, target_slices, p_lo=15.0, p_hi=85.0):\n",
    "    if Z <= 0 or target_slices <= 0:\n",
    "        return np.array([], dtype=int)\n",
    "            \n",
    "    # percentiles spaced linearly from p_lo to p_hi (inclusive)\n",
    "    ps = np.linspace(p_lo, p_hi, num=target_slices)\n",
    "    idx = np.round((ps / 100.0) * (Z - 1)).astype(int)\n",
    "    idx = np.clip(idx, 0, Z - 1)\n",
    "\n",
    "    # de-duplicate while preserving order (can happen if Z is small) \n",
    "    uniq, first_pos = np.unique(idx, return_index=True) \n",
    "    idx = idx[np.sort(first_pos)] # Same values as np.unique, but derived from original order\n",
    "    # pad if needed\n",
    "    if len(idx) < target_slices:\n",
    "        pad = np.full(target_slices - len(idx), idx[-1] if len(idx) else 0, dtype=int)\n",
    "        idx = np.concatenate([idx, pad])\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T21:57:44.195163Z",
     "iopub.status.busy": "2025-12-04T21:57:44.194400Z",
     "iopub.status.idle": "2025-12-04T21:57:44.206547Z",
     "shell.execute_reply": "2025-12-04T21:57:44.205504Z",
     "shell.execute_reply.started": "2025-12-04T21:57:44.195132Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_series(series_dir, target_hw=256):\n",
    "    series_dir = Path(series_dir)\n",
    "    files = sorted(series_dir.glob(\"*.dcm\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No DICOMs in {series_dir}\")\n",
    "\n",
    "    # slice ordering \n",
    "    try:\n",
    "        paths, n = _physical_sorted_paths(files)\n",
    "    except Exception:\n",
    "        def sort_key(fp):\n",
    "            ds = pydicom.dcmread(str(fp), stop_before_pixels=True)\n",
    "            return float(getattr(ds, \"InstanceNumber\", 0))\n",
    "        paths = sorted(files, key=sort_key)\n",
    "\n",
    "    # Read all slices, no subsampling \n",
    "    vol_slices = []\n",
    "    sop_uids = []\n",
    "\n",
    "    for fp in paths:\n",
    "        ds = pydicom.dcmread(str(fp), force=True, defer_size=\"1 KB\")\n",
    "        sop = getattr(ds, \"SOPInstanceUID\", None)\n",
    "        if sop is None:\n",
    "            continue  # skip slices with no SOP\n",
    "        sop_uids.append(sop)\n",
    "\n",
    "        arr = ds.pixel_array\n",
    "        if arr.ndim == 3 and arr.shape[0] > 1 and arr.shape[-1] != 3:\n",
    "            arr = arr.mean(axis=0)\n",
    "        if arr.ndim == 3 and arr.shape[-1] == 3:\n",
    "            arr = arr.mean(axis=-1)\n",
    "\n",
    "        arr_small = cv2.resize(arr, (target_hw, target_hw), interpolation=cv2.INTER_AREA)\n",
    "        img = arr_small.astype(np.float32)\n",
    "        img *= float(getattr(ds, \"RescaleSlope\", 1.0))\n",
    "        img += float(getattr(ds, \"RescaleIntercept\", 0.0))\n",
    "\n",
    "        if getattr(ds, \"PhotometricInterpretation\", \"MONOCHROME2\") == \"MONOCHROME1\":\n",
    "            mmax, mmin = float(img.max()), float(img.min())\n",
    "            img *= -1.0\n",
    "            img += (mmax + mmin)\n",
    "\n",
    "        vol_slices.append(img)\n",
    "\n",
    "    vol = np.stack(vol_slices).astype(np.float32)\n",
    "\n",
    "    modality = getattr(pydicom.dcmread(str(paths[0]), stop_before_pixels=True),\n",
    "                       \"Modality\", \"Unknown\")\n",
    "\n",
    "    if modality in {\"CT\", \"CTA\"}:\n",
    "        # lo, hi = -100.0, 500.0\n",
    "        vals = vol[vol != 0]\n",
    "        lo, hi = np.percentile(vals, [5, 95]) if vals.size >= 100 else np.percentile(vol, [5,95])\n",
    "    else:\n",
    "        vals = vol[vol != 0]\n",
    "        lo, hi = np.percentile(vals, [5, 95]) if vals.size >= 100 else np.percentile(vol, [5,95])\n",
    "\n",
    "    np.clip(vol, lo, hi, out=vol)\n",
    "    vol -= lo\n",
    "    vol /= (hi - lo + 1e-6)\n",
    "\n",
    "    return vol.astype(np.float16), sop_uids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T21:57:48.549183Z",
     "iopub.status.busy": "2025-12-04T21:57:48.548794Z",
     "iopub.status.idle": "2025-12-04T21:57:50.901046Z",
     "shell.execute_reply": "2025-12-04T21:57:50.900142Z",
     "shell.execute_reply.started": "2025-12-04T21:57:48.549152Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3726 series ready\n",
      "/kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.11798530207335736916333444551246253735\n",
      "[0 0 0 0 0 0 1 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "data_path = Path(\"/kaggle/input/rsna-intracranial-aneurysm-detection\")\n",
    "series_dir = data_path / \"series\"\n",
    "\n",
    "# read labels\n",
    "df = pd.read_csv(data_path / \"train.csv\")\n",
    "\n",
    "# add the folder path for each series\n",
    "df['path'] = df['SeriesInstanceUID'].apply(lambda uid: series_dir / str(uid))\n",
    "\n",
    "# keep only rows whose folder actually exists\n",
    "df = df[df['path'].apply(lambda p: p.exists())].reset_index(drop = True)\n",
    "\n",
    "# take a balanced sample\n",
    "n_per_class = 1863\n",
    "if n_per_class is not None:\n",
    "    pos = df[df[\"Aneurysm Present\"] == 1].sample(min(n_per_class, (df[\"Aneurysm Present\"] == 1).sum()), random_state = 0)\n",
    "    neg = df[df[\"Aneurysm Present\"] == 0].sample(min(n_per_class, (df[\"Aneurysm Present\"] == 0).sum()), random_state = 0)\n",
    "    df = pd.concat([pos, neg]).sample(frac = 1, random_state = 0).reset_index(drop = True)\n",
    "\n",
    "# ensure labels are numerical ints\n",
    "df[LABEL_COLS] = (\n",
    "    df[LABEL_COLS].apply(pd.to_numeric, errors = 'coerce').fillna(0).astype(int)\n",
    ")\n",
    "\n",
    "series_list = list(zip(df['path'].tolist(), df[LABEL_COLS].to_numpy(dtype=int)))\n",
    "\n",
    "print(len(series_list), \"series ready\")\n",
    "print(series_list[0][0]) # a path\n",
    "print(series_list[0][1]) # a label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T21:57:53.790386Z",
     "iopub.status.busy": "2025-12-04T21:57:53.789300Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total subjects: 3726\n",
      "Shard 2/4: 931 subjects\n"
     ]
    }
   ],
   "source": [
    "# Canonical order so all shards pick the SAME subjects deterministically\n",
    "series_list = sorted(series_list, key=lambda t: t[0].name) # t = (Path, labels)\n",
    "N = len(series_list)\n",
    "print(\"Total subjects:\", N)\n",
    "\n",
    "K = 4  # total shards\n",
    "shard_id = 2 # CHANGE per notebook\n",
    "\n",
    "# modulo sharding \n",
    "idxs = [i for i in range(N) if i % K == shard_id]\n",
    "print(f\"Shard {shard_id}/{K}: {len(idxs)} subjects\")\n",
    "\n",
    "H, W = 256, 256\n",
    "L = len(LABEL_COLS)\n",
    "\n",
    "h5_path = f\"/kaggle/working/dataset_shard_{shard_id}_all_slices.h5\"\n",
    "\n",
    "with h5py.File(h5_path, \"w\") as f:\n",
    "    x_grp = f.create_group(\"x\")\n",
    "    y_ds = f.create_dataset(\"y\", shape=(len(idxs), L), dtype=\"int16\", compression=\"gzip\")\n",
    "    uid_ds = f.create_dataset(\"uid\", shape=(len(idxs),), dtype=h5py.string_dtype(encoding=\"utf-8\"))\n",
    "    zlen_ds = f.create_dataset(\"z_len\", shape=(len(idxs),), dtype=\"int16\")\n",
    "    yslice_grp = f.create_group(\"y_slice\")\n",
    "    \n",
    "    for j, i in enumerate(idxs):\n",
    "        sd, y = series_list[i] # sd is a Path to the series directory\n",
    "        series_uid = sd.name # this matches SeriesInstanceUID in CSVs\n",
    "\n",
    "        vol, sop_uids = preprocess_series(sd, target_hw=256)\n",
    "        Z_i = vol.shape[0]\n",
    "\n",
    "        # build per-slice y_slice (Z_i, 14) \n",
    "        y_slice = np.zeros((Z_i, 14), dtype=\"int16\")\n",
    "\n",
    "        rows = loc_by_series.get(series_uid, None)\n",
    "        if rows is not None:\n",
    "            # map SOPInstanceUID -> slice index in vol\n",
    "            uid_to_z = {uid: z for z, uid in enumerate(sop_uids)}\n",
    "\n",
    "            for _, row_loc in rows.iterrows():\n",
    "                sop = row_loc[\"SOPInstanceUID\"]\n",
    "                if sop not in uid_to_z:\n",
    "                    continue  \n",
    "\n",
    "                z = uid_to_z[sop] # slice index 0..Z_i-1\n",
    "                loc_name = row_loc[\"location\"]\n",
    "\n",
    "                if loc_name not in SLICE_LOCATION_TO_IDX:\n",
    "                    # unexpected text; skip\n",
    "                    continue\n",
    "\n",
    "                loc_idx = SLICE_LOCATION_TO_IDX[loc_name] # 0..12\n",
    "\n",
    "                # set the one-hot location and \"aneurysm present on this slice\"\n",
    "                y_slice[z, loc_idx] = 1\n",
    "                y_slice[z, 13] = 1\n",
    "\n",
    "        # write data to HDF5 \n",
    "        x_grp.create_dataset(\n",
    "            name = str(j),\n",
    "            data = vol,\n",
    "            dtype = \"float16\",\n",
    "            compression = \"gzip\"\n",
    "        )\n",
    "\n",
    "        # subject-level y \n",
    "        y_ds[j] = y.astype(\"int16\")\n",
    "        uid_ds[j] = series_uid\n",
    "        zlen_ds[j] = Z_i\n",
    "\n",
    "        # per-slice labels\n",
    "        yslice_grp.create_dataset(\n",
    "            name = str(j),\n",
    "            data = y_slice,\n",
    "            dtype = \"int16\",\n",
    "            compression = \"gzip\"\n",
    "        )\n",
    "\n",
    "        del vol, y_slice\n",
    "        if (j + 1) % 10 == 0:\n",
    "            gc.collect()\n",
    "\n",
    "        if (j + 1) % 50 == 0:\n",
    "            f.flush()\n",
    "            print(f\"Shard {shard_id}: wrote {j+1}/{len(idxs)}\")\n",
    "\n",
    "print(\"Saved shard to:\", h5_path)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13851420,
     "sourceId": 99552,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
