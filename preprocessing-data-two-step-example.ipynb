{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":99552,"databundleVersionId":13851420,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nimport os, json, pathlib, shutil\nimport gc\nimport re\nimport cv2\nimport math\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport pydicom\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision.models as models\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, TensorDataset, DataLoader, random_split, Subset\nimport timm\nfrom collections import defaultdict\nfrom typing import List, Tuple\nimport shutil\nimport matplotlib.pyplot as plt\nimport random\nimport sklearn\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\nfrom IPython.display import display\nimport joblib\nfrom joblib import Parallel, delayed\nfrom pathlib import Path\nimport h5py, numpy as np\n\nROOT = \"/kaggle/input/rsna-intracranial-aneurysm-detection\"  \nsys.path.append(ROOT)  # parent of kaggle_evaluation\n\nimport kaggle_evaluation.rsna_inference_server as rsna_eval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T21:57:21.911116Z","iopub.execute_input":"2025-12-04T21:57:21.911462Z","iopub.status.idle":"2025-12-04T21:57:28.597621Z","shell.execute_reply.started":"2025-12-04T21:57:21.911435Z","shell.execute_reply":"2025-12-04T21:57:28.596822Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"ID_COL = 'SeriesInstanceUID'\n\nLABEL_COLS = [\n    'Left Infraclinoid Internal Carotid Artery',\n    'Right Infraclinoid Internal Carotid Artery',\n    'Left Supraclinoid Internal Carotid Artery',\n    'Right Supraclinoid Internal Carotid Artery',\n    'Left Middle Cerebral Artery',\n    'Right Middle Cerebral Artery',\n    'Anterior Communicating Artery',\n    'Left Anterior Cerebral Artery',\n    'Right Anterior Cerebral Artery',\n    'Left Posterior Communicating Artery',\n    'Right Posterior Communicating Artery',\n    'Basilar Tip',\n    'Other Posterior Circulation',\n    'Aneurysm Present',\n]\n\n# per-slice label order (0..12) \nSLICE_LABEL_NAMES = [\n    \"Other Posterior Circulation\",                    \n    \"Basilar Tip\",                                    \n    \"Right Posterior Communicating Artery\",           \n    \"Left Posterior Communicating Artery\",            \n    \"Right Infraclinoid Internal Carotid Artery\",     \n    \"Left Infraclinoid Internal Carotid Artery\",      \n    \"Right Supraclinoid Internal Carotid Artery\",     \n    \"Left Supraclinoid Internal Carotid Artery\",      \n    \"Right Middle Cerebral Artery\",                   \n    \"Left Middle Cerebral Artery\",                    \n    \"Right Anterior Cerebral Artery\",                 \n    \"Left Anterior Cerebral Artery\",                  \n    \"Anterior Communicating Artery\",                  \n]\n\n# text -> index 0..12 for per-slice labels\nSLICE_LOCATION_TO_IDX = {name: i for i, name in enumerate(SLICE_LABEL_NAMES)}\n\n# load train_localizers.csv and group by SeriesInstanceUID \nloc_df = pd.read_csv(f\"{ROOT}/train_localizers.csv\")\nloc_by_series = {k: g for k, g in loc_df.groupby(\"SeriesInstanceUID\")}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T21:57:32.213554Z","iopub.execute_input":"2025-12-04T21:57:32.214110Z","iopub.status.idle":"2025-12-04T21:57:32.259172Z","shell.execute_reply.started":"2025-12-04T21:57:32.214082Z","shell.execute_reply":"2025-12-04T21:57:32.258396Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def _unit(v):\n    v = np.asarray(v, float)\n    n = np.linalg.norm(v)\n    return v / (n + 1e-12)\n\ndef _slice_normal_from_iop(iop):\n    r = _unit(iop[:3]) # row direction\n    c = _unit(iop[3:]) # col direction\n    n = np.cross(r, c) # slice normal\n    return _unit(n)\n\ndef _scalar_pos_along_normal(ds, n):\n    # project IPP onto the normal (dot product) (https://discovery.ucl.ac.uk/id/eprint/10146893/1/geometry_medim.pdf)\n    p = np.asarray(getattr(ds, \"ImagePositionPatient\", [0,0,0]), float) # or default to [0,0,0]\n    return float(np.dot(n, p))\n\ndef _physical_sorted_paths(files):\n    # read first file to get IOP (row/col vectors)\n    ds0 = pydicom.dcmread(str(files[0]), stop_before_pixels=True) # assuming IOP is the same for all slices\n    iop = np.asarray(getattr(ds0, \"ImageOrientationPatient\", [1,0,0,0,1,0]), float) # or default to [1,0,0,0,1,0]\n    n = _slice_normal_from_iop(iop)\n\n    keyed = []\n    for fp in files:\n        ds = pydicom.dcmread(str(fp), stop_before_pixels=True)\n        s = _scalar_pos_along_normal(ds, n)\n        keyed.append((s, fp)) # pair of tuples (call this pair t)\n    keyed.sort(key=lambda t: t[0]) # sort by first thing in pair (s)    \n    return [pair[1] for pair in keyed], n # once sorted, only need file\n\ndef _percentile_slice_indices(Z, target_slices, p_lo=15.0, p_hi=85.0):\n    if Z <= 0 or target_slices <= 0:\n        return np.array([], dtype=int)\n            \n    # percentiles spaced linearly from p_lo to p_hi (inclusive)\n    ps = np.linspace(p_lo, p_hi, num=target_slices)\n    idx = np.round((ps / 100.0) * (Z - 1)).astype(int)\n    idx = np.clip(idx, 0, Z - 1)\n\n    # de-duplicate while preserving order (can happen if Z is small) \n    uniq, first_pos = np.unique(idx, return_index=True) \n    idx = idx[np.sort(first_pos)] # Same values as np.unique, but derived from original order\n    # pad if needed\n    if len(idx) < target_slices:\n        pad = np.full(target_slices - len(idx), idx[-1] if len(idx) else 0, dtype=int)\n        idx = np.concatenate([idx, pad])\n    return idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T21:57:40.443837Z","iopub.execute_input":"2025-12-04T21:57:40.445014Z","iopub.status.idle":"2025-12-04T21:57:40.455491Z","shell.execute_reply.started":"2025-12-04T21:57:40.444978Z","shell.execute_reply":"2025-12-04T21:57:40.454615Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def preprocess_series(series_dir, target_hw=256):\n    series_dir = Path(series_dir)\n    files = sorted(series_dir.glob(\"*.dcm\"))\n    if not files:\n        raise FileNotFoundError(f\"No DICOMs in {series_dir}\")\n\n    # slice ordering \n    try:\n        paths, n = _physical_sorted_paths(files)\n    except Exception:\n        def sort_key(fp):\n            ds = pydicom.dcmread(str(fp), stop_before_pixels=True)\n            return float(getattr(ds, \"InstanceNumber\", 0))\n        paths = sorted(files, key=sort_key)\n\n    # Read all slices, no subsampling \n    vol_slices = []\n    sop_uids = []\n\n    for fp in paths:\n        ds = pydicom.dcmread(str(fp), force=True, defer_size=\"1 KB\")\n        sop = getattr(ds, \"SOPInstanceUID\", None)\n        if sop is None:\n            continue  # skip slices with no SOP\n        sop_uids.append(sop)\n\n        arr = ds.pixel_array\n        if arr.ndim == 3 and arr.shape[0] > 1 and arr.shape[-1] != 3:\n            arr = arr.mean(axis=0)\n        if arr.ndim == 3 and arr.shape[-1] == 3:\n            arr = arr.mean(axis=-1)\n\n        arr_small = cv2.resize(arr, (target_hw, target_hw), interpolation=cv2.INTER_AREA)\n        img = arr_small.astype(np.float32)\n        img *= float(getattr(ds, \"RescaleSlope\", 1.0))\n        img += float(getattr(ds, \"RescaleIntercept\", 0.0))\n\n        if getattr(ds, \"PhotometricInterpretation\", \"MONOCHROME2\") == \"MONOCHROME1\":\n            mmax, mmin = float(img.max()), float(img.min())\n            img *= -1.0\n            img += (mmax + mmin)\n\n        vol_slices.append(img)\n\n    vol = np.stack(vol_slices).astype(np.float32)\n\n    modality = getattr(pydicom.dcmread(str(paths[0]), stop_before_pixels=True),\n                       \"Modality\", \"Unknown\")\n\n    if modality in {\"CT\", \"CTA\"}:\n        # lo, hi = -100.0, 500.0\n        vals = vol[vol != 0]\n        lo, hi = np.percentile(vals, [5, 95]) if vals.size >= 100 else np.percentile(vol, [5,95])\n    else:\n        vals = vol[vol != 0]\n        lo, hi = np.percentile(vals, [5, 95]) if vals.size >= 100 else np.percentile(vol, [5,95])\n\n    np.clip(vol, lo, hi, out=vol)\n    vol -= lo\n    vol /= (hi - lo + 1e-6)\n\n    return vol.astype(np.float16), sop_uids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T21:57:44.194400Z","iopub.execute_input":"2025-12-04T21:57:44.195163Z","iopub.status.idle":"2025-12-04T21:57:44.206547Z","shell.execute_reply.started":"2025-12-04T21:57:44.195132Z","shell.execute_reply":"2025-12-04T21:57:44.205504Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"data_path = Path(\"/kaggle/input/rsna-intracranial-aneurysm-detection\")\nseries_dir = data_path / \"series\"\n\n# read labels\ndf = pd.read_csv(data_path / \"train.csv\")\n\n# add the folder path for each series\ndf['path'] = df['SeriesInstanceUID'].apply(lambda uid: series_dir / str(uid))\n\n# keep only rows whose folder actually exists\ndf = df[df['path'].apply(lambda p: p.exists())].reset_index(drop = True)\n\n# take a balanced sample\nn_per_class = 1863\nif n_per_class is not None:\n    pos = df[df[\"Aneurysm Present\"] == 1].sample(min(n_per_class, (df[\"Aneurysm Present\"] == 1).sum()), random_state = 0)\n    neg = df[df[\"Aneurysm Present\"] == 0].sample(min(n_per_class, (df[\"Aneurysm Present\"] == 0).sum()), random_state = 0)\n    df = pd.concat([pos, neg]).sample(frac = 1, random_state = 0).reset_index(drop = True)\n\n# ensure labels are numerical ints\ndf[LABEL_COLS] = (\n    df[LABEL_COLS].apply(pd.to_numeric, errors = 'coerce').fillna(0).astype(int)\n)\n\nseries_list = list(zip(df['path'].tolist(), df[LABEL_COLS].to_numpy(dtype=int)))\n\nprint(len(series_list), \"series ready\")\nprint(series_list[0][0]) # a path\nprint(series_list[0][1]) # a label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T21:57:48.548794Z","iopub.execute_input":"2025-12-04T21:57:48.549183Z","iopub.status.idle":"2025-12-04T21:57:50.901046Z","shell.execute_reply.started":"2025-12-04T21:57:48.549152Z","shell.execute_reply":"2025-12-04T21:57:50.900142Z"}},"outputs":[{"name":"stdout","text":"3726 series ready\n/kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.11798530207335736916333444551246253735\n[0 0 0 0 0 0 1 0 0 0 0 0 0 1]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Canonical order so all shards pick the SAME subjects deterministically\nseries_list = sorted(series_list, key=lambda t: t[0].name) # t = (Path, labels)\nN = len(series_list)\nprint(\"Total subjects:\", N)\n\nK = 4  # total shards\nshard_id = 2 # CHANGE per notebook\n\n# modulo sharding \nidxs = [i for i in range(N) if i % K == shard_id]\nprint(f\"Shard {shard_id}/{K}: {len(idxs)} subjects\")\n\nH, W = 256, 256\nL = len(LABEL_COLS)\n\nh5_path = f\"/kaggle/working/dataset_shard_{shard_id}_all_slices.h5\"\n\nwith h5py.File(h5_path, \"w\") as f:\n    x_grp = f.create_group(\"x\")\n    y_ds = f.create_dataset(\"y\", shape=(len(idxs), L), dtype=\"int16\", compression=\"gzip\")\n    uid_ds = f.create_dataset(\"uid\", shape=(len(idxs),), dtype=h5py.string_dtype(encoding=\"utf-8\"))\n    zlen_ds = f.create_dataset(\"z_len\", shape=(len(idxs),), dtype=\"int16\")\n    yslice_grp = f.create_group(\"y_slice\")\n    \n    for j, i in enumerate(idxs):\n        sd, y = series_list[i] # sd is a Path to the series directory\n        series_uid = sd.name # this matches SeriesInstanceUID in CSVs\n\n        vol, sop_uids = preprocess_series(sd, target_hw=256)\n        Z_i = vol.shape[0]\n\n        # build per-slice y_slice (Z_i, 14) \n        y_slice = np.zeros((Z_i, 14), dtype=\"int16\")\n\n        rows = loc_by_series.get(series_uid, None)\n        if rows is not None:\n            # map SOPInstanceUID -> slice index in vol\n            uid_to_z = {uid: z for z, uid in enumerate(sop_uids)}\n\n            for _, row_loc in rows.iterrows():\n                sop = row_loc[\"SOPInstanceUID\"]\n                if sop not in uid_to_z:\n                    continue  \n\n                z = uid_to_z[sop] # slice index 0..Z_i-1\n                loc_name = row_loc[\"location\"]\n\n                if loc_name not in SLICE_LOCATION_TO_IDX:\n                    # unexpected text; skip\n                    continue\n\n                loc_idx = SLICE_LOCATION_TO_IDX[loc_name] # 0..12\n\n                # set the one-hot location and \"aneurysm present on this slice\"\n                y_slice[z, loc_idx] = 1\n                y_slice[z, 13] = 1\n\n        # write data to HDF5 \n        x_grp.create_dataset(\n            name = str(j),\n            data = vol,\n            dtype = \"float16\",\n            compression = \"gzip\"\n        )\n\n        # subject-level y \n        y_ds[j] = y.astype(\"int16\")\n        uid_ds[j] = series_uid\n        zlen_ds[j] = Z_i\n\n        # per-slice labels\n        yslice_grp.create_dataset(\n            name = str(j),\n            data = y_slice,\n            dtype = \"int16\",\n            compression = \"gzip\"\n        )\n\n        del vol, y_slice\n        if (j + 1) % 10 == 0:\n            gc.collect()\n\n        if (j + 1) % 50 == 0:\n            f.flush()\n            print(f\"Shard {shard_id}: wrote {j+1}/{len(idxs)}\")\n\nprint(\"Saved shard to:\", h5_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T21:57:53.789300Z","iopub.execute_input":"2025-12-04T21:57:53.790386Z","iopub.status.idle":"2025-12-04T22:20:19.540646Z","shell.execute_reply.started":"2025-12-04T21:57:53.790346Z","shell.execute_reply":"2025-12-04T22:20:19.538927Z"}},"outputs":[{"name":"stdout","text":"Total subjects: 3726\nShard 2/4: 931 subjects\nShard 2: wrote 50/931\nShard 2: wrote 100/931\nShard 2: wrote 150/931\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_208/1910371558.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mseries_uid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;31m# this matches SeriesInstanceUID in CSVs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mvol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msop_uids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_series\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_hw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mZ_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_208/1269265396.py\u001b[0m in \u001b[0;36mpreprocess_series\u001b[0;34m(series_dir, target_hw)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# slice ordering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mpaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_physical_sorted_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0msort_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_208/1788451983.py\u001b[0m in \u001b[0;36m_physical_sorted_paths\u001b[0;34m(files)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mkeyed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydicom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdcmread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_before_pixels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_scalar_pos_along_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# pair of tuples (call this pair t)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydicom/filereader.py\u001b[0m in \u001b[0;36mdcmread\u001b[0;34m(fp, defer_size, stop_before_pixels, force, specific_tags)\u001b[0m\n\u001b[1;32m   1040\u001b[0m         \u001b[0mcaller_owns_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Reading file '{fp}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m     elif (\n\u001b[1;32m   1044\u001b[0m         \u001b[0mfp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":6}]}