{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "condacolab.check()\n",
        "!conda create -y -n colab_env python=3.10\n",
        "!conda install -y -n colab_env numpy=1.19.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ui33bQ3l_StI",
        "outputId": "32315466-90cf-4593-ef9b-c22c457ae65c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ¨ðŸ°âœ¨ Everything looks OK!\n",
            "âœ¨ðŸ°âœ¨ Everything looks OK!\n",
            "Channels:\n",
            " - conda-forge\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "    current version: 24.11.2\n",
            "    latest version: 25.9.1\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c conda-forge conda\n",
            "\n",
            "\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local/envs/colab_env\n",
            "\n",
            "  added / updated specs:\n",
            "    - python=3.6\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    ca-certificates-2025.10.5  |       hbd8a1cb_0         152 KB  conda-forge\n",
            "    ld_impl_linux-64-2.44      |       h1aa0949_4         725 KB  conda-forge\n",
            "    libffi-3.4.6               |       h2dba641_1          56 KB  conda-forge\n",
            "    libgcc-15.2.0              |       h767d61c_7         803 KB  conda-forge\n",
            "    libgcc-ng-15.2.0           |       h69a702a_7          29 KB  conda-forge\n",
            "    libgomp-15.2.0             |       h767d61c_7         437 KB  conda-forge\n",
            "    liblzma-5.8.1              |       hb9d3cd8_2         110 KB  conda-forge\n",
            "    liblzma-devel-5.8.1        |       hb9d3cd8_2         430 KB  conda-forge\n",
            "    libnsl-2.0.1               |       hb9d3cd8_1          33 KB  conda-forge\n",
            "    libsqlite-3.50.4           |       h0c1763c_0         911 KB  conda-forge\n",
            "    libstdcxx-15.2.0           |       h8f9b012_7         3.7 MB  conda-forge\n",
            "    libstdcxx-ng-15.2.0        |       h4852527_7          29 KB  conda-forge\n",
            "    ncurses-6.5                |       h2d0b736_3         871 KB  conda-forge\n",
            "    openssl-1.1.1w             |       hd590300_0         1.9 MB  conda-forge\n",
            "    pip-21.3.1                 |     pyhd8ed1ab_0         1.2 MB  conda-forge\n",
            "    python-3.6.15              |hb7a2778_0_cpython        38.4 MB  conda-forge\n",
            "    python_abi-3.6             |          2_cp36m           4 KB  conda-forge\n",
            "    readline-8.2               |       h8c095d6_2         276 KB  conda-forge\n",
            "    setuptools-58.0.4          |   py36h5fab9bb_2         966 KB  conda-forge\n",
            "    sqlite-3.50.4              |       hbc0de68_0         162 KB  conda-forge\n",
            "    tk-8.6.13                  |noxft_hd72426e_102         3.1 MB  conda-forge\n",
            "    wheel-0.37.1               |     pyhd8ed1ab_0          31 KB  conda-forge\n",
            "    xz-5.8.1                   |       hbcc6ac9_2          23 KB  conda-forge\n",
            "    xz-gpl-tools-5.8.1         |       hbcc6ac9_2          33 KB  conda-forge\n",
            "    xz-tools-5.8.1             |       hb9d3cd8_2          94 KB  conda-forge\n",
            "    zstd-1.5.7                 |       hb8e6e7a_2         554 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        54.8 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge \n",
            "  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-2_gnu \n",
            "  ca-certificates    conda-forge/noarch::ca-certificates-2025.10.5-hbd8a1cb_0 \n",
            "  ld_impl_linux-64   conda-forge/linux-64::ld_impl_linux-64-2.44-h1aa0949_4 \n",
            "  libffi             conda-forge/linux-64::libffi-3.4.6-h2dba641_1 \n",
            "  libgcc             conda-forge/linux-64::libgcc-15.2.0-h767d61c_7 \n",
            "  libgcc-ng          conda-forge/linux-64::libgcc-ng-15.2.0-h69a702a_7 \n",
            "  libgomp            conda-forge/linux-64::libgomp-15.2.0-h767d61c_7 \n",
            "  liblzma            conda-forge/linux-64::liblzma-5.8.1-hb9d3cd8_2 \n",
            "  liblzma-devel      conda-forge/linux-64::liblzma-devel-5.8.1-hb9d3cd8_2 \n",
            "  libnsl             conda-forge/linux-64::libnsl-2.0.1-hb9d3cd8_1 \n",
            "  libsqlite          conda-forge/linux-64::libsqlite-3.50.4-h0c1763c_0 \n",
            "  libstdcxx          conda-forge/linux-64::libstdcxx-15.2.0-h8f9b012_7 \n",
            "  libstdcxx-ng       conda-forge/linux-64::libstdcxx-ng-15.2.0-h4852527_7 \n",
            "  libzlib            conda-forge/linux-64::libzlib-1.3.1-hb9d3cd8_2 \n",
            "  ncurses            conda-forge/linux-64::ncurses-6.5-h2d0b736_3 \n",
            "  openssl            conda-forge/linux-64::openssl-1.1.1w-hd590300_0 \n",
            "  pip                conda-forge/noarch::pip-21.3.1-pyhd8ed1ab_0 \n",
            "  python             conda-forge/linux-64::python-3.6.15-hb7a2778_0_cpython \n",
            "  python_abi         conda-forge/linux-64::python_abi-3.6-2_cp36m \n",
            "  readline           conda-forge/linux-64::readline-8.2-h8c095d6_2 \n",
            "  setuptools         conda-forge/linux-64::setuptools-58.0.4-py36h5fab9bb_2 \n",
            "  sqlite             conda-forge/linux-64::sqlite-3.50.4-hbc0de68_0 \n",
            "  tk                 conda-forge/linux-64::tk-8.6.13-noxft_hd72426e_102 \n",
            "  wheel              conda-forge/noarch::wheel-0.37.1-pyhd8ed1ab_0 \n",
            "  xz                 conda-forge/linux-64::xz-5.8.1-hbcc6ac9_2 \n",
            "  xz-gpl-tools       conda-forge/linux-64::xz-gpl-tools-5.8.1-hbcc6ac9_2 \n",
            "  xz-tools           conda-forge/linux-64::xz-tools-5.8.1-hb9d3cd8_2 \n",
            "  zstd               conda-forge/linux-64::zstd-1.5.7-hb8e6e7a_2 \n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "python-3.6.15        | 38.4 MB   | :   0% 0/1 [00:00<?, ?it/s]\n",
            "libstdcxx-15.2.0     | 3.7 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "tk-8.6.13            | 3.1 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "openssl-1.1.1w       | 1.9 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "pip-21.3.1           | 1.2 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "setuptools-58.0.4    | 966 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libsqlite-3.50.4     | 911 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ncurses-6.5          | 871 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-15.2.0        | 803 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ld_impl_linux-64-2.4 | 725 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "zstd-1.5.7           | 554 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgomp-15.2.0       | 437 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "liblzma-devel-5.8.1  | 430 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "readline-8.2         | 276 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sqlite-3.50.4        | 162 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ca-certificates-2025 | 152 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "liblzma-5.8.1        | 110 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "xz-tools-5.8.1       | 94 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libffi-3.4.6         | 56 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "tk-8.6.13            | 3.1 MB    | :   0% 0.004987209317899284/1 [00:00<00:21, 21.48s/it]\u001b[A\u001b[A\n",
            "python-3.6.15        | 38.4 MB   | :   0% 0.00040738642512046545/1 [00:00<04:37, 277.21s/it]\n",
            "\n",
            "\n",
            "\n",
            "pip-21.3.1           | 1.2 MB    | :   1% 0.013012954974889858/1 [00:00<00:09,  9.82s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "openssl-1.1.1w       | 1.9 MB    | :   1% 0.008376235295320576/1 [00:00<00:18, 19.01s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "pip-21.3.1           | 1.2 MB    | : 100% 1.0/1 [00:00<00:00,  9.82s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "tk-8.6.13            | 3.1 MB    | : 100% 1.0/1 [00:00<00:00,  5.59it/s]                 \u001b[A\u001b[A\n",
            "\n",
            "python-3.6.15        | 38.4 MB   | :  11% 0.10551308410620056/1 [00:00<00:01,  1.70s/it]    \n",
            "libstdcxx-15.2.0     | 3.7 MB    | : 100% 1.0/1 [00:00<00:00,  4.48it/s]                 \u001b[A\n",
            "libstdcxx-15.2.0     | 3.7 MB    | : 100% 1.0/1 [00:00<00:00,  4.48it/s]\u001b[A\n",
            "\n",
            "\n",
            "openssl-1.1.1w       | 1.9 MB    | : 100% 1.0/1 [00:00<00:00,  4.62it/s]                 \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "openssl-1.1.1w       | 1.9 MB    | : 100% 1.0/1 [00:00<00:00,  4.62it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "setuptools-58.0.4    | 966 KB    | :   2% 0.016569361446502017/1 [00:00<00:15, 16.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "python-3.6.15        | 38.4 MB   | :  24% 0.24443185507227927/1 [00:00<00:00,  1.06s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ncurses-6.5          | 871 KB    | :   2% 0.018375108367605347/1 [00:00<00:16, 17.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "setuptools-58.0.4    | 966 KB    | : 100% 1.0/1 [00:00<00:00, 16.04s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libsqlite-3.50.4     | 911 KB    | : 100% 1.0/1 [00:00<00:00, 15.82s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-15.2.0        | 803 KB    | :   2% 0.019918497553954036/1 [00:00<00:16, 17.34s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ncurses-6.5          | 871 KB    | : 100% 1.0/1 [00:00<00:00, 17.12s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "python-3.6.15        | 38.4 MB   | :  34% 0.34261198352631145/1 [00:00<00:00,  1.04s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-15.2.0        | 803 KB    | : 100% 1.0/1 [00:00<00:00, 17.34s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ld_impl_linux-64-2.4 | 725 KB    | : 100% 1.0/1 [00:00<00:00, 18.01s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgomp-15.2.0       | 437 KB    | :   4% 0.03657804201206022/1 [00:00<00:11, 12.03s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgomp-15.2.0       | 437 KB    | : 100% 1.0/1 [00:00<00:00, 12.03s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "python-3.6.15        | 38.4 MB   | :  49% 0.4949745065213655/1 [00:00<00:00,  1.14it/s] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "zstd-1.5.7           | 554 KB    | : 100% 1.0/1 [00:00<00:00, 16.68s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "liblzma-devel-5.8.1  | 430 KB    | :   4% 0.037247537897732955/1 [00:00<00:13, 13.94s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "liblzma-devel-5.8.1  | 430 KB    | : 100% 1.0/1 [00:00<00:00, 13.94s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sqlite-3.50.4        | 162 KB    | :  10% 0.09856045430209405/1 [00:00<00:05,  5.84s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "python-3.6.15        | 38.4 MB   | :  69% 0.6937790819801526/1 [00:00<00:00,  1.42it/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "liblzma-5.8.1        | 110 KB    | :  15% 0.1451272875440679/1 [00:00<00:03,  4.29s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "liblzma-5.8.1        | 110 KB    | : 100% 1.0/1 [00:00<00:00,  4.29s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ca-certificates-2025 | 152 KB    | :  11% 0.10508828981379925/1 [00:00<00:05,  6.16s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ca-certificates-2025 | 152 KB    | : 100% 1.0/1 [00:00<00:00,  6.16s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "xz-tools-5.8.1       | 94 KB     | :  17% 0.1699003453174743/1 [00:00<00:03,  3.84s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "xz-tools-5.8.1       | 94 KB     | : 100% 1.0/1 [00:00<00:00,  3.84s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libffi-3.4.6         | 56 KB     | :  29% 0.2852715337871955/1 [00:00<00:01,  2.42s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "python-3.6.15        | 38.4 MB   | :  87% 0.8685478583568323/1 [00:00<00:00,  1.53it/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "readline-8.2         | 276 KB    | :   6% 0.05800056641178136/1 [00:00<00:13, 14.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "readline-8.2         | 276 KB    | : 100% 1.0/1 [00:00<00:00, 14.51s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "tk-8.6.13            | 3.1 MB    | : 100% 1.0/1 [00:00<00:00,  5.59it/s]\u001b[A\u001b[A\n",
            "python-3.6.15        | 38.4 MB   | : 100% 1.0/1 [00:01<00:00,  1.53it/s]               \n",
            "\n",
            "\n",
            "\n",
            "pip-21.3.1           | 1.2 MB    | : 100% 1.0/1 [00:01<00:00,  1.13s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "pip-21.3.1           | 1.2 MB    | : 100% 1.0/1 [00:01<00:00,  1.13s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "openssl-1.1.1w       | 1.9 MB    | : 100% 1.0/1 [00:01<00:00,  4.62it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libsqlite-3.50.4     | 911 KB    | : 100% 1.0/1 [00:01<00:00,  1.19s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libsqlite-3.50.4     | 911 KB    | : 100% 1.0/1 [00:01<00:00,  1.19s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "setuptools-58.0.4    | 966 KB    | : 100% 1.0/1 [00:01<00:00,  1.57s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "setuptools-58.0.4    | 966 KB    | : 100% 1.0/1 [00:01<00:00,  1.57s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-15.2.0        | 803 KB    | : 100% 1.0/1 [00:01<00:00,  1.61s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-15.2.0        | 803 KB    | : 100% 1.0/1 [00:01<00:00,  1.61s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ld_impl_linux-64-2.4 | 725 KB    | : 100% 1.0/1 [00:01<00:00,  1.66s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ld_impl_linux-64-2.4 | 725 KB    | : 100% 1.0/1 [00:01<00:00,  1.66s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgomp-15.2.0       | 437 KB    | : 100% 1.0/1 [00:01<00:00,  1.70s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgomp-15.2.0       | 437 KB    | : 100% 1.0/1 [00:01<00:00,  1.70s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "zstd-1.5.7           | 554 KB    | : 100% 1.0/1 [00:01<00:00,  1.72s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "zstd-1.5.7           | 554 KB    | : 100% 1.0/1 [00:01<00:00,  1.72s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "liblzma-devel-5.8.1  | 430 KB    | : 100% 1.0/1 [00:02<00:00,  1.87s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "liblzma-devel-5.8.1  | 430 KB    | : 100% 1.0/1 [00:02<00:00,  1.87s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sqlite-3.50.4        | 162 KB    | : 100% 1.0/1 [00:02<00:00,  1.91s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sqlite-3.50.4        | 162 KB    | : 100% 1.0/1 [00:02<00:00,  1.91s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "liblzma-5.8.1        | 110 KB    | : 100% 1.0/1 [00:02<00:00,  1.94s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "liblzma-5.8.1        | 110 KB    | : 100% 1.0/1 [00:02<00:00,  1.94s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ca-certificates-2025 | 152 KB    | : 100% 1.0/1 [00:02<00:00,  1.93s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ca-certificates-2025 | 152 KB    | : 100% 1.0/1 [00:02<00:00,  1.93s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "xz-tools-5.8.1       | 94 KB     | : 100% 1.0/1 [00:02<00:00,  2.00s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "xz-tools-5.8.1       | 94 KB     | : 100% 1.0/1 [00:02<00:00,  2.00s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libffi-3.4.6         | 56 KB     | : 100% 1.0/1 [00:02<00:00,  2.09s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libffi-3.4.6         | 56 KB     | : 100% 1.0/1 [00:02<00:00,  2.09s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "readline-8.2         | 276 KB    | : 100% 1.0/1 [00:02<00:00,  2.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "readline-8.2         | 276 KB    | : 100% 1.0/1 [00:02<00:00,  2.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ncurses-6.5          | 871 KB    | : 100% 1.0/1 [00:02<00:00,  2.47s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "python-3.6.15        | 38.4 MB   | : 100% 1.0/1 [00:07<00:00,  1.53it/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \n",
            "                                                                        \u001b[A\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Preparing transaction: - \b\b\\ \b\b| \b\bdone\n",
            "Verifying transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Executing transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "#\n",
            "# To activate this environment, use\n",
            "#\n",
            "#     $ conda activate colab_env\n",
            "#\n",
            "# To deactivate an active environment, use\n",
            "#\n",
            "#     $ conda deactivate\n",
            "\n",
            "Channels:\n",
            " - conda-forge\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\b\\ \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "    current version: 24.11.2\n",
            "    latest version: 25.9.1\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c conda-forge conda\n",
            "\n",
            "\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local/envs/colab_env\n",
            "\n",
            "  added / updated specs:\n",
            "    - numpy=1.19.5\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    libblas-3.9.0              |38_h4a7cf45_openblas          17 KB  conda-forge\n",
            "    libcblas-3.9.0             |38_h0358290_openblas          17 KB  conda-forge\n",
            "    libgfortran-15.2.0         |       h69a702a_7          29 KB  conda-forge\n",
            "    libgfortran5-15.2.0        |       hcd61629_7         1.5 MB  conda-forge\n",
            "    liblapack-3.9.0            |38_h47877c9_openblas          17 KB  conda-forge\n",
            "    libopenblas-0.3.30         |pthreads_h94d23a6_3         5.6 MB  conda-forge\n",
            "    numpy-1.19.5               |   py36hfc0c790_2         5.3 MB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        12.5 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  libblas            conda-forge/linux-64::libblas-3.9.0-38_h4a7cf45_openblas \n",
            "  libcblas           conda-forge/linux-64::libcblas-3.9.0-38_h0358290_openblas \n",
            "  libgfortran        conda-forge/linux-64::libgfortran-15.2.0-h69a702a_7 \n",
            "  libgfortran5       conda-forge/linux-64::libgfortran5-15.2.0-hcd61629_7 \n",
            "  liblapack          conda-forge/linux-64::liblapack-3.9.0-38_h47877c9_openblas \n",
            "  libopenblas        conda-forge/linux-64::libopenblas-0.3.30-pthreads_h94d23a6_3 \n",
            "  numpy              conda-forge/linux-64::numpy-1.19.5-py36hfc0c790_2 \n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "libopenblas-0.3.30   | 5.6 MB    | :   0% 0/1 [00:00<?, ?it/s]\n",
            "numpy-1.19.5         | 5.3 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "libgfortran5-15.2.0  | 1.5 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libgfortran-15.2.0   | 29 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libblas-3.9.0        | 17 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcblas-3.9.0       | 17 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "liblapack-3.9.0      | 17 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "libgfortran5-15.2.0  | 1.5 MB    | :   9% 0.09375631851817/1 [00:00<00:01,  1.12s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libblas-3.9.0        | 17 KB     | :  94% 0.9350530761328616/1 [00:00<00:00,  9.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libblas-3.9.0        | 17 KB     | : 100% 1.0/1 [00:00<00:00,  9.03it/s]               \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libblas-3.9.0        | 17 KB     | : 100% 1.0/1 [00:00<00:00,  9.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "libgfortran5-15.2.0  | 1.5 MB    | : 100% 1.0/1 [00:00<00:00,  1.12s/it]             \u001b[A\u001b[A\n",
            "libopenblas-0.3.30   | 5.6 MB    | :   0% 0.0027683686174732654/1 [00:00<00:59, 59.46s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcblas-3.9.0       | 17 KB     | :  94% 0.936068102610981/1 [00:00<00:00,  5.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcblas-3.9.0       | 17 KB     | : 100% 1.0/1 [00:00<00:00,  5.70it/s]              \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libgfortran-15.2.0   | 29 KB     | :  56% 0.5596584116140051/1 [00:00<00:00,  3.24it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libgfortran-15.2.0   | 29 KB     | : 100% 1.0/1 [00:00<00:00,  3.24it/s]               \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "liblapack-3.9.0      | 17 KB     | :  94% 0.9361750757099594/1 [00:00<00:00,  4.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "liblapack-3.9.0      | 17 KB     | : 100% 1.0/1 [00:00<00:00,  4.70it/s]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcblas-3.9.0       | 17 KB     | : 100% 1.0/1 [00:00<00:00,  5.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "libgfortran5-15.2.0  | 1.5 MB    | : 100% 1.0/1 [00:00<00:00,  4.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "libopenblas-0.3.30   | 5.6 MB    | :  52% 0.5176849314675006/1 [00:00<00:00,  2.40it/s]   \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "liblapack-3.9.0      | 17 KB     | : 100% 1.0/1 [00:00<00:00,  4.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libgfortran-15.2.0   | 29 KB     | : 100% 1.0/1 [00:00<00:00,  3.48it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libgfortran-15.2.0   | 29 KB     | : 100% 1.0/1 [00:00<00:00,  3.48it/s]\u001b[A\u001b[A\u001b[A\n",
            "numpy-1.19.5         | 5.3 MB    | : 100% 1.0/1 [00:00<00:00,  3.88it/s]                  \u001b[A\n",
            "libopenblas-0.3.30   | 5.6 MB    | : 100% 1.0/1 [00:01<00:00,  1.20it/s]\n",
            "                                                                        \n",
            "                                                                        \u001b[A\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Preparing transaction: - \b\bdone\n",
            "Verifying transaction: | \b\b/ \b\bdone\n",
            "Executing transaction: \\ \b\bdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda run -n colab_env python -c \"import numpy; print(numpy.__version__)\""
      ],
      "metadata": {
        "id": "S4c_I_giBSnq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "076ceb0c-eb78-4ebe-f542-d590f9132380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.19.5\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda install -y -n colab_env ipykernel\n",
        "!conda run -n colab_env python -m ipykernel install --user --name=colab_env --display-name \"Python (colab_env)\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13rfd0gYRM93",
        "outputId": "9113b7d3-d315-45b5-a669-13209f1dcdae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Channels:\n",
            " - conda-forge\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Solving environment: | \b\b/ \b\b- \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "    current version: 24.11.2\n",
            "    latest version: 25.9.1\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c conda-forge conda\n",
            "\n",
            "\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local/envs/colab_env\n",
            "\n",
            "  added / updated specs:\n",
            "    - ipykernel\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    backcall-0.2.0             |     pyh9f0ad1d_0          13 KB  conda-forge\n",
            "    backports-1.0              |     pyhd8ed1ab_4           7 KB  conda-forge\n",
            "    backports.functools_lru_cache-2.0.0|     pyhd8ed1ab_0          12 KB  conda-forge\n",
            "    decorator-5.1.1            |     pyhd8ed1ab_0          12 KB  conda-forge\n",
            "    entrypoints-0.4            |     pyhd8ed1ab_0           9 KB  conda-forge\n",
            "    ipykernel-5.5.5            |   py36hcb3619a_0         166 KB  conda-forge\n",
            "    ipython-7.16.1             |   py36he448a4c_2         1.1 MB  conda-forge\n",
            "    ipython_genutils-0.2.0     |     pyhd8ed1ab_1          27 KB  conda-forge\n",
            "    jedi-0.17.2                |   py36h5fab9bb_1         957 KB  conda-forge\n",
            "    jupyter_client-7.1.2       |     pyhd8ed1ab_0          90 KB  conda-forge\n",
            "    jupyter_core-4.8.1         |   py36h5fab9bb_0          78 KB  conda-forge\n",
            "    libsodium-1.0.18           |       h36c2ea0_1         366 KB  conda-forge\n",
            "    nest-asyncio-1.6.0         |     pyhd8ed1ab_0          11 KB  conda-forge\n",
            "    parso-0.7.1                |     pyh9f0ad1d_0          70 KB  conda-forge\n",
            "    pexpect-4.8.0              |     pyh1a96a4e_2          48 KB  conda-forge\n",
            "    pickleshare-0.7.5          |          py_1003           9 KB  conda-forge\n",
            "    prompt-toolkit-3.0.36      |     pyha770c72_0         265 KB  conda-forge\n",
            "    ptyprocess-0.7.0           |     pyhd3deb0d_0          16 KB  conda-forge\n",
            "    pygments-2.14.0            |     pyhd8ed1ab_0         805 KB  conda-forge\n",
            "    python-dateutil-2.8.2      |     pyhd8ed1ab_0         240 KB  conda-forge\n",
            "    pyzmq-22.3.0               |   py36h7068817_0         500 KB  conda-forge\n",
            "    six-1.16.0                 |     pyh6c4a22f_0          14 KB  conda-forge\n",
            "    tornado-6.1                |   py36h8f6f2f9_1         643 KB  conda-forge\n",
            "    traitlets-4.3.3            |     pyhd8ed1ab_2          62 KB  conda-forge\n",
            "    wcwidth-0.2.10             |     pyhd8ed1ab_0          32 KB  conda-forge\n",
            "    zeromq-4.3.5               |       h59595ed_1         335 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:         5.8 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  backcall           conda-forge/noarch::backcall-0.2.0-pyh9f0ad1d_0 \n",
            "  backports          conda-forge/noarch::backports-1.0-pyhd8ed1ab_4 \n",
            "  backports.functoo~ conda-forge/noarch::backports.functools_lru_cache-2.0.0-pyhd8ed1ab_0 \n",
            "  decorator          conda-forge/noarch::decorator-5.1.1-pyhd8ed1ab_0 \n",
            "  entrypoints        conda-forge/noarch::entrypoints-0.4-pyhd8ed1ab_0 \n",
            "  ipykernel          conda-forge/linux-64::ipykernel-5.5.5-py36hcb3619a_0 \n",
            "  ipython            conda-forge/linux-64::ipython-7.16.1-py36he448a4c_2 \n",
            "  ipython_genutils   conda-forge/noarch::ipython_genutils-0.2.0-pyhd8ed1ab_1 \n",
            "  jedi               conda-forge/linux-64::jedi-0.17.2-py36h5fab9bb_1 \n",
            "  jupyter_client     conda-forge/noarch::jupyter_client-7.1.2-pyhd8ed1ab_0 \n",
            "  jupyter_core       conda-forge/linux-64::jupyter_core-4.8.1-py36h5fab9bb_0 \n",
            "  libsodium          conda-forge/linux-64::libsodium-1.0.18-h36c2ea0_1 \n",
            "  nest-asyncio       conda-forge/noarch::nest-asyncio-1.6.0-pyhd8ed1ab_0 \n",
            "  parso              conda-forge/noarch::parso-0.7.1-pyh9f0ad1d_0 \n",
            "  pexpect            conda-forge/noarch::pexpect-4.8.0-pyh1a96a4e_2 \n",
            "  pickleshare        conda-forge/noarch::pickleshare-0.7.5-py_1003 \n",
            "  prompt-toolkit     conda-forge/noarch::prompt-toolkit-3.0.36-pyha770c72_0 \n",
            "  ptyprocess         conda-forge/noarch::ptyprocess-0.7.0-pyhd3deb0d_0 \n",
            "  pygments           conda-forge/noarch::pygments-2.14.0-pyhd8ed1ab_0 \n",
            "  python-dateutil    conda-forge/noarch::python-dateutil-2.8.2-pyhd8ed1ab_0 \n",
            "  pyzmq              conda-forge/linux-64::pyzmq-22.3.0-py36h7068817_0 \n",
            "  six                conda-forge/noarch::six-1.16.0-pyh6c4a22f_0 \n",
            "  tornado            conda-forge/linux-64::tornado-6.1-py36h8f6f2f9_1 \n",
            "  traitlets          conda-forge/noarch::traitlets-4.3.3-pyhd8ed1ab_2 \n",
            "  wcwidth            conda-forge/noarch::wcwidth-0.2.10-pyhd8ed1ab_0 \n",
            "  zeromq             conda-forge/linux-64::zeromq-4.3.5-h59595ed_1 \n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "ipython-7.16.1       | 1.1 MB    | :   0% 0/1 [00:00<?, ?it/s]\n",
            "jedi-0.17.2          | 957 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "pygments-2.14.0      | 805 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "tornado-6.1          | 643 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "pyzmq-22.3.0         | 500 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libsodium-1.0.18     | 366 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "zeromq-4.3.5         | 335 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "prompt-toolkit-3.0.3 | 265 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "python-dateutil-2.8. | 240 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ipykernel-5.5.5      | 166 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "jupyter_client-7.1.2 | 90 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "jupyter_core-4.8.1   | 78 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "parso-0.7.1          | 70 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "traitlets-4.3.3      | 62 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pexpect-4.8.0        | 48 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "wcwidth-0.2.10       | 32 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ipython_genutils-0.2 | 27 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ptyprocess-0.7.0     | 16 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "six-1.16.0           | 14 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pygments-2.14.0      | 805 KB    | :   2% 0.01987874258217392/1 [00:00<00:05,  5.85s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "tornado-6.1          | 643 KB    | :   2% 0.024864288998796547/1 [00:00<00:04,  4.92s/it]\u001b[A\u001b[A\u001b[A\n",
            "jedi-0.17.2          | 957 KB    | :   2% 0.016719288613862397/1 [00:00<00:07,  7.91s/it]\u001b[A\n",
            "\n",
            "\n",
            "tornado-6.1          | 643 KB    | : 100% 1.0/1 [00:00<00:00,  4.92s/it]                 \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pygments-2.14.0      | 805 KB    | : 100% 1.0/1 [00:00<00:00,  5.85s/it]                \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "ipython-7.16.1       | 1.1 MB    | :   1% 0.014028850591542446/1 [00:00<00:13, 13.37s/it]\n",
            "jedi-0.17.2          | 957 KB    | : 100% 1.0/1 [00:00<00:00,  7.91s/it]                 \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "pyzmq-22.3.0         | 500 KB    | : 100% 1.0/1 [00:00<00:00,  5.09s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ipython-7.16.1       | 1.1 MB    | : 100% 1.0/1 [00:00<00:00, 13.37s/it]                 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "zeromq-4.3.5         | 335 KB    | : 100% 1.0/1 [00:00<00:00,  4.78s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "prompt-toolkit-3.0.3 | 265 KB    | :   6% 0.06033955732331602/1 [00:00<00:04,  4.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libsodium-1.0.18     | 366 KB    | :   4% 0.0436907831754218/1 [00:00<00:06,  6.52s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "prompt-toolkit-3.0.3 | 265 KB    | : 100% 1.0/1 [00:00<00:00,  4.60s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libsodium-1.0.18     | 366 KB    | : 100% 1.0/1 [00:00<00:00,  6.52s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "jupyter_client-7.1.2 | 90 KB     | :  18% 0.1777083604494772/1 [00:00<00:01,  1.80s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "jupyter_client-7.1.2 | 90 KB     | : 100% 1.0/1 [00:00<00:00,  1.80s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "python-dateutil-2.8. | 240 KB    | :   7% 0.06660514580038783/1 [00:00<00:04,  5.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "python-dateutil-2.8. | 240 KB    | : 100% 1.0/1 [00:00<00:00,  5.02s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ipykernel-5.5.5      | 166 KB    | :  10% 0.09621968909482784/1 [00:00<00:03,  3.71s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ipykernel-5.5.5      | 166 KB    | : 100% 1.0/1 [00:00<00:00,  3.71s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "traitlets-4.3.3      | 62 KB     | :  26% 0.2569313762388659/1 [00:00<00:01,  1.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "traitlets-4.3.3      | 62 KB     | : 100% 1.0/1 [00:00<00:00,  1.51s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "jupyter_core-4.8.1   | 78 KB     | :  20% 0.20408315790784867/1 [00:00<00:01,  1.97s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "jupyter_core-4.8.1   | 78 KB     | : 100% 1.0/1 [00:00<00:00,  1.97s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "wcwidth-0.2.10       | 32 KB     | :  50% 0.5032404705593267/1 [00:00<00:00,  1.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "wcwidth-0.2.10       | 32 KB     | : 100% 1.0/1 [00:00<00:00,  1.17it/s]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pygments-2.14.0      | 805 KB    | : 100% 1.0/1 [00:00<00:00,  2.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "pygments-2.14.0      | 805 KB    | : 100% 1.0/1 [00:00<00:00,  2.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ptyprocess-0.7.0     | 16 KB     | :  99% 0.9902091139852532/1 [00:00<00:00,  2.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ptyprocess-0.7.0     | 16 KB     | : 100% 1.0/1 [00:00<00:00,  2.15it/s]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ipython_genutils-0.2 | 27 KB     | :  58% 0.5838084378563284/1 [00:00<00:00,  1.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ipython_genutils-0.2 | 27 KB     | : 100% 1.0/1 [00:00<00:00,  1.24it/s]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "parso-0.7.1          | 70 KB     | :  23% 0.22835161465665027/1 [00:00<00:01,  2.09s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "parso-0.7.1          | 70 KB     | : 100% 1.0/1 [00:00<00:00,  2.09s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pexpect-4.8.0        | 48 KB     | :  34% 0.33587535875358754/1 [00:00<00:00,  1.46s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pexpect-4.8.0        | 48 KB     | : 100% 1.0/1 [00:00<00:00,  1.46s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "six-1.16.0           | 14 KB     | : 100% 1.0/1 [00:00<00:00,  2.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "six-1.16.0           | 14 KB     | : 100% 1.0/1 [00:00<00:00,  2.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "tornado-6.1          | 643 KB    | : 100% 1.0/1 [00:00<00:00,  1.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "tornado-6.1          | 643 KB    | : 100% 1.0/1 [00:00<00:00,  1.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "pyzmq-22.3.0         | 500 KB    | : 100% 1.0/1 [00:00<00:00,  1.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "ipython-7.16.1       | 1.1 MB    | : 100% 1.0/1 [00:01<00:00,  1.36s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "zeromq-4.3.5         | 335 KB    | : 100% 1.0/1 [00:01<00:00,  1.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "zeromq-4.3.5         | 335 KB    | : 100% 1.0/1 [00:01<00:00,  1.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "prompt-toolkit-3.0.3 | 265 KB    | : 100% 1.0/1 [00:01<00:00,  1.48s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "prompt-toolkit-3.0.3 | 265 KB    | : 100% 1.0/1 [00:01<00:00,  1.48s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "jedi-0.17.2          | 957 KB    | : 100% 1.0/1 [00:01<00:00,  1.58s/it]\u001b[A\n",
            "jedi-0.17.2          | 957 KB    | : 100% 1.0/1 [00:01<00:00,  1.58s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libsodium-1.0.18     | 366 KB    | : 100% 1.0/1 [00:01<00:00,  1.57s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libsodium-1.0.18     | 366 KB    | : 100% 1.0/1 [00:01<00:00,  1.57s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "jupyter_client-7.1.2 | 90 KB     | : 100% 1.0/1 [00:01<00:00,  1.65s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "jupyter_client-7.1.2 | 90 KB     | : 100% 1.0/1 [00:01<00:00,  1.65s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "python-dateutil-2.8. | 240 KB    | : 100% 1.0/1 [00:01<00:00,  1.62s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "python-dateutil-2.8. | 240 KB    | : 100% 1.0/1 [00:01<00:00,  1.62s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "traitlets-4.3.3      | 62 KB     | : 100% 1.0/1 [00:01<00:00,  1.74s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "traitlets-4.3.3      | 62 KB     | : 100% 1.0/1 [00:01<00:00,  1.74s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ipykernel-5.5.5      | 166 KB    | : 100% 1.0/1 [00:01<00:00,  1.71s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ipykernel-5.5.5      | 166 KB    | : 100% 1.0/1 [00:01<00:00,  1.71s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "wcwidth-0.2.10       | 32 KB     | : 100% 1.0/1 [00:01<00:00,  1.95s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "wcwidth-0.2.10       | 32 KB     | : 100% 1.0/1 [00:01<00:00,  1.95s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "jupyter_core-4.8.1   | 78 KB     | : 100% 1.0/1 [00:01<00:00,  1.78s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "jupyter_core-4.8.1   | 78 KB     | : 100% 1.0/1 [00:01<00:00,  1.78s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ptyprocess-0.7.0     | 16 KB     | : 100% 1.0/1 [00:01<00:00,  2.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ipython_genutils-0.2 | 27 KB     | : 100% 1.0/1 [00:01<00:00,  2.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ipython_genutils-0.2 | 27 KB     | : 100% 1.0/1 [00:01<00:00,  2.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pexpect-4.8.0        | 48 KB     | : 100% 1.0/1 [00:01<00:00,  1.87s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pexpect-4.8.0        | 48 KB     | : 100% 1.0/1 [00:01<00:00,  1.87s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "parso-0.7.1          | 70 KB     | : 100% 1.0/1 [00:01<00:00,  1.82s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "parso-0.7.1          | 70 KB     | : 100% 1.0/1 [00:01<00:00,  1.82s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "six-1.16.0           | 14 KB     | : 100% 1.0/1 [00:01<00:00,  2.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \n",
            "                                                                        \u001b[A\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Preparing transaction: - \b\bdone\n",
            "Verifying transaction: | \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Executing transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Installed kernelspec colab_env in /root/.local/share/jupyter/kernels/colab_env\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda run -n colab_env python -m notebook"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEqYGfUnRT0r",
        "outputId": "95c3e9d1-7f33-4a71-e023-c83f1ce3e29e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/envs/colab_env/bin/python: No module named notebook\n",
            "\n",
            "ERROR conda.cli.main_run:execute(125): `conda run python -m notebook` failed. (See above for error)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(np.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUsh154yPxXq",
        "outputId": "4e987536-f8b7-426c-e45c-74164d302f0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XHTF-_Z1tS2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55cc8994-0eea-443b-c3d2-eb4cc0c2cf48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/.shortcut-targets-by-id/1MtNc6sd4xgIRS7Ti7LH0WRLdzziuFPGk/comp540/datasets\n",
            "/content/drive/My Drive/Transformers Captioning\n"
          ]
        }
      ],
      "source": [
        "# This mounts your Google Drive to the Colab VM.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# TODO: Enter the foldername in your Drive where you have saved the unzipped\n",
        "# assignment folder, e.g. 'comp540/assignments/image_cap/'\n",
        "# CHANGE THIS!!\n",
        "FOLDERNAME = 'Transformers Captioning'\n",
        "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "\n",
        "# Now that we've mounted your Drive, this ensures that\n",
        "# the Python interpreter of the Colab VM can load\n",
        "# python files from within it.\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
        "\n",
        "# This downloads the COCO dataset to your Drive\n",
        "# if it doesn't already exist.\n",
        "%cd /content/drive/My\\ Drive/$FOLDERNAME/comp540/datasets/\n",
        "# !bash get_datasets.sh\n",
        "!bash get_coco_dataset.sh\n",
        "%cd /content/drive/My\\ Drive/$FOLDERNAME\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import imp  # noqa: F401\n",
        "except ModuleNotFoundError:\n",
        "    import types, importlib, sys\n",
        "    imp = types.ModuleType(\"imp\")\n",
        "    imp.reload = importlib.reload\n",
        "    sys.modules[\"imp\"] = imp\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "OfVbA8K7SCIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRvYckCI7qyX",
        "tags": [
          "pdf-title"
        ]
      },
      "source": [
        "# Image Captioning with Transformers\n",
        "In this notebook you will implement key pieces of a transformer decoder to perform image captioning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXCykho37qya",
        "tags": [
          "pdf-ignore"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95db13d9-44a4-40f3-cebc-62b23448b63d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "# Setup cell.\n",
        "import time, os, json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from comp540.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
        "from comp540.transformer_layers import *\n",
        "from comp540.captioning_solver_transformer import CaptioningSolverTransformer\n",
        "from comp540.classifiers.transformer import CaptioningTransformer\n",
        "from comp540.coco_utils import load_coco_data, sample_coco_minibatch, decode_captions\n",
        "from comp540.image_utils import image_from_url\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # Set default size of plots.\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "def rel_error(x, y):\n",
        "    \"\"\" returns relative error \"\"\"\n",
        "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fczxReWM7qyd"
      },
      "source": [
        "# COCO Dataset\n",
        "As in the previous notebooks, we will use the COCO dataset for captioning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCKchFVJ7qye",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ef29711-09b5-4bf8-9eca-5c6a9e225f05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "base dir  /content/drive/.shortcut-targets-by-id/1MtNc6sd4xgIRS7Ti7LH0WRLdzziuFPGk/comp540/datasets/coco_captioning\n",
            "train_captions <class 'numpy.ndarray'> (400135, 17) int32\n",
            "train_image_idxs <class 'numpy.ndarray'> (400135,) int32\n",
            "val_captions <class 'numpy.ndarray'> (195954, 17) int32\n",
            "val_image_idxs <class 'numpy.ndarray'> (195954,) int32\n",
            "train_features <class 'numpy.ndarray'> (82783, 512) float32\n",
            "val_features <class 'numpy.ndarray'> (40504, 512) float32\n",
            "idx_to_word <class 'list'> 1004\n",
            "word_to_idx <class 'dict'> 1004\n",
            "train_urls <class 'numpy.ndarray'> (82783,) <U63\n",
            "val_urls <class 'numpy.ndarray'> (40504,) <U63\n"
          ]
        }
      ],
      "source": [
        "# Load COCO data from disk into a dictionary.\n",
        "data = load_coco_data(pca_features=True)\n",
        "\n",
        "# Print out all the keys and values from the data dictionary.\n",
        "for k, v in data.items():\n",
        "    if type(v) == np.ndarray:\n",
        "        print(k, type(v), v.shape, v.dtype)\n",
        "    else:\n",
        "        print(k, type(v), len(v))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go1XDgIn7qyg"
      },
      "source": [
        "# Transformer\n",
        "As you have seen, RNNs are incredibly powerful but often slow to train. Further, RNNs struggle to encode long-range dependencies (though LSTMs are one way of mitigating the issue). In 2017, Vaswani et al introduced the Transformer in their paper [\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762) to a) introduce parallelism and b) allow models to learn long-range dependencies. The paper not only led to famous models like BERT and GPT in the natural language processing community, but also an explosion of interest across fields, including vision. While here we introduce the model in the context of image captioning, the idea of attention itself is much more general.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqPMDm4F9m0v"
      },
      "source": [
        "# Transformer: Multi-Headed Attention\n",
        "\n",
        "### Dot-Product Attention\n",
        "\n",
        "Recall that attention can be viewed as an operation on a query $q\\in\\mathbb{R}^d$, a set of value vectors $\\{v_1,\\dots,v_n\\}, v_i\\in\\mathbb{R}^d$, and a set of key vectors $\\{k_1,\\dots,k_n\\}, k_i \\in \\mathbb{R}^d$, specified as"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91g6XLTr7qyi"
      },
      "source": [
        "\\begin{align}\n",
        "c &= \\sum_{i=1}^{n} v_i \\alpha_i \\\\\n",
        "\\alpha_i &= \\frac{\\exp(k_i^\\top q)}{\\sum_{j=1}^{n} \\exp(k_j^\\top q)} \\\\\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D2wcdeS7qyj"
      },
      "source": [
        "where $\\alpha_i$ are frequently called the \"attention weights\", and the output $c\\in\\mathbb{R}^d$ is a correspondingly weighted average over the value vectors.\n",
        "\n",
        "### Self-Attention\n",
        "In Transformers, we perform self-attention, which means that the values, keys and query are derived from the input $X \\in \\mathbb{R}^{\\ell \\times d}$, where $\\ell$ is our sequence length. Specifically, we learn parameter matrices $V,K,Q \\in \\mathbb{R}^{d\\times d}$ to map our input $X$ as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2JDsjFU7qyl"
      },
      "source": [
        "\\begin{align}\n",
        "v_i = Vx_i\\ \\ i \\in \\{1,\\dots,\\ell\\}\\\\\n",
        "k_i = Kx_i\\ \\ i \\in \\{1,\\dots,\\ell\\}\\\\\n",
        "q_i = Qx_i\\ \\ i \\in \\{1,\\dots,\\ell\\}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5GNjXoW7qyn"
      },
      "source": [
        "### Multi-Headed Scaled Dot-Product Attention\n",
        "In the case of multi-headed attention, we learn a parameter matrix for each head, which gives the model more expressivity to attend to different parts of the input. Let $h$ be number of heads, and $Y_i$ be the attention output of head $i$. Thus we learn individual matrices $Q_i$, $K_i$ and $V_i$. To keep our overall computation the same as the single-headed case, we choose $Q_i \\in \\mathbb{R}^{d\\times d/h}$, $K_i \\in \\mathbb{R}^{d\\times d/h}$ and $V_i \\in \\mathbb{R}^{d\\times d/h}$. Adding in a scaling term $\\frac{1}{\\sqrt{d/h}}$ to our simple dot-product attention above, we have"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRyHp98f7qyo"
      },
      "source": [
        "\\begin{equation}\n",
        "Y_i = \\text{softmax}\\bigg(\\frac{(XQ_i)(XK_i)^\\top}{\\sqrt{d/h}}\\bigg)(XV_i)\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzKHMc3g7qyo"
      },
      "source": [
        "where $Y_i\\in\\mathbb{R}^{\\ell \\times d/h}$, where $\\ell$ is our sequence length.\n",
        "\n",
        "In our implementation, we apply dropout to the attention weights (though in practice it could be used at any step):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJFFO4uu7qyp"
      },
      "source": [
        "\\begin{equation}\n",
        "Y_i = \\text{dropout}\\bigg(\\text{softmax}\\bigg(\\frac{(XQ_i)(XK_i)^\\top}{\\sqrt{d/h}}\\bigg)\\bigg)(XV_i)\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KlyhrBW7qyp"
      },
      "source": [
        "Finally, then the output of the self-attention is a linear transformation of the concatenation of the heads:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFR0osPf7qyq"
      },
      "source": [
        "\\begin{equation}\n",
        "Y = [Y_1;\\dots;Y_h]A\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4G9fKfW7qyq"
      },
      "source": [
        "were $A \\in\\mathbb{R}^{d\\times d}$ and $[Y_1;\\dots;Y_h]\\in\\mathbb{R}^{\\ell \\times d}$.\n",
        "\n",
        "Implement multi-headed scaled dot-product attention in the `MultiHeadAttention` class in the file `comp540/transformer_layers.py`. The code below will check your implementation. The relative error should be less than `e-3`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeixCEKF7qyr",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "439c380d-7795-42e7-e427-545eb834337e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "self_attn_output error:  0.4493818531828122\n",
            "masked_self_attn_output error:  1.0\n",
            "attn_output error:  1.0\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(231)\n",
        "\n",
        "# Choose dimensions such that they are all unique for easier debugging:\n",
        "# Specifically, the following values correspond to N=1, H=2, T=3, E//H=4, and E=8.\n",
        "batch_size = 1\n",
        "sequence_length = 3\n",
        "embed_dim = 8\n",
        "attn = MultiHeadAttention(embed_dim, num_heads=2)\n",
        "\n",
        "# Self-attention.\n",
        "data = torch.randn(batch_size, sequence_length, embed_dim)\n",
        "self_attn_output = attn(query=data, key=data, value=data)\n",
        "\n",
        "# Masked self-attention.\n",
        "mask = torch.randn(sequence_length, sequence_length) < 0.5\n",
        "masked_self_attn_output = attn(query=data, key=data, value=data, attn_mask=mask)\n",
        "\n",
        "# Attention using two inputs.\n",
        "other_data = torch.randn(batch_size, sequence_length, embed_dim)\n",
        "attn_output = attn(query=data, key=other_data, value=other_data)\n",
        "\n",
        "expected_self_attn_output = np.asarray([[\n",
        "[-0.2494,  0.1396,  0.4323, -0.2411, -0.1547,  0.2329, -0.1936,\n",
        "          -0.1444],\n",
        "         [-0.1997,  0.1746,  0.7377, -0.3549, -0.2657,  0.2693, -0.2541,\n",
        "          -0.2476],\n",
        "         [-0.0625,  0.1503,  0.7572, -0.3974, -0.1681,  0.2168, -0.2478,\n",
        "          -0.3038]]])\n",
        "\n",
        "expected_masked_self_attn_output = np.asarray([[\n",
        "[-0.1347,  0.1934,  0.8628, -0.4903, -0.2614,  0.2798, -0.2586,\n",
        "          -0.3019],\n",
        "         [-0.1013,  0.3111,  0.5783, -0.3248, -0.3842,  0.1482, -0.3628,\n",
        "          -0.1496],\n",
        "         [-0.2071,  0.1669,  0.7097, -0.3152, -0.3136,  0.2520, -0.2774,\n",
        "          -0.2208]]])\n",
        "\n",
        "expected_attn_output = np.asarray([[\n",
        "[-0.1980,  0.4083,  0.1968, -0.3477,  0.0321,  0.4258, -0.8972,\n",
        "          -0.2744],\n",
        "         [-0.1603,  0.4155,  0.2295, -0.3485, -0.0341,  0.3929, -0.8248,\n",
        "          -0.2767],\n",
        "         [-0.0908,  0.4113,  0.3017, -0.3539, -0.1020,  0.3784, -0.7189,\n",
        "          -0.2912]]])\n",
        "\n",
        "print('self_attn_output error: ', rel_error(expected_self_attn_output, self_attn_output.detach().numpy()))\n",
        "print('masked_self_attn_output error: ', rel_error(expected_masked_self_attn_output, masked_self_attn_output.detach().numpy()))\n",
        "print('attn_output error: ', rel_error(expected_attn_output, attn_output.detach().numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcDBRnqL9m0w"
      },
      "source": [
        "# Positional Encoding\n",
        "\n",
        "While transformers are able to easily attend to any part of their input, the attention mechanism has no concept of token order. However, for many tasks (especially natural language processing), relative token order is very important. To recover this, the authors add a positional encoding to the embeddings of individual word tokens.\n",
        "\n",
        "Let us define a matrix $P \\in \\mathbb{R}^{l\\times d}$, where $P_{ij} = $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zawv8vzV7qyt"
      },
      "source": [
        "$$\n",
        "\\begin{cases}\n",
        "\\text{sin}\\left(i \\cdot 10000^{-\\frac{j}{d}}\\right) & \\text{if j is even} \\\\\n",
        "\\text{cos}\\left(i \\cdot 10000^{-\\frac{(j-1)}{d}}\\right) & \\text{otherwise} \\\\\n",
        "\\end{cases}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0fhlupT7qyt"
      },
      "source": [
        "Rather than directly passing an input $X \\in \\mathbb{R}^{l\\times d}$ to our network, we instead pass $X + P$.\n",
        "\n",
        "Implement this layer in `PositionalEncoding` in `comp540/transformer_layers.py`. Once you are done, run the following to perform a simple test of your implementation. You should see errors on the order of `e-3` or less."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gi7px0XK7qyu",
        "tags": []
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(231)\n",
        "\n",
        "batch_size = 1\n",
        "sequence_length = 2\n",
        "embed_dim = 6\n",
        "data = torch.randn(batch_size, sequence_length, embed_dim)\n",
        "\n",
        "pos_encoder = PositionalEncoding(embed_dim)\n",
        "output = pos_encoder(data)\n",
        "\n",
        "expected_pe_output = np.asarray([[[-1.2340,  1.1127,  1.6978, -0.0865, -0.0000,  1.2728],\n",
        "                                  [ 0.9028, -0.4781,  0.5535,  0.8133,  1.2644,  1.7034]]])\n",
        "\n",
        "print('pe_output error: ', rel_error(expected_pe_output, output.detach().numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDoSUJ7y7qyv",
        "tags": [
          "pdf-inline"
        ]
      },
      "source": [
        "# Inline Question 1\n",
        "\n",
        "Several key design decisions were made in designing the scaled dot product attention we introduced above. Explain why the following choices were beneficial:\n",
        "1. Using multiple attention heads as opposed to one.\n",
        "2. Dividing by $\\sqrt{d/h}$ before applying the softmax function. Recall that $d$ is the feature dimension and $h$ is the number of heads.\n",
        "3. Adding a linear transformation to the output of the attention operation.\n",
        "\n",
        "Only one or two sentences per choice is necessary, but be sure to be specific in addressing what would have happened without each given implementation detail, why such a situation would be suboptimal, and how the proposed implementation improves the situation.\n",
        "\n",
        "**Your Answer:**\n",
        "- Using multiple heads\n",
        "- Dividing by $\\sqrt{d/h}$\n",
        "- Adding a linear transform to the output of the attention operation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkfRjeETn2TU"
      },
      "source": [
        "# Transformer Decoder Block\n",
        "\n",
        "The Transformer decoder layer has three main components: (1) a self-attention module that processes the input sequence of vectors, (2) a cross-attention module that incorporates additional context (e.g., image features in our case), and (3) a feedforward module that independently processes each vector in the sequence. Complete the implementation of `TransformerDecoderLayer` in `comp540/transformer_layers.py` and test it below. The relative error should be less than 1e-6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHyiLXjwpU4q"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(231)\n",
        "np.random.seed(231)\n",
        "\n",
        "N, T, TM, D = 1, 4, 5, 12\n",
        "\n",
        "decoder_layer = TransformerDecoderLayer(D, 2, 4*D)\n",
        "tgt = torch.randn(N, T, D)\n",
        "memory = torch.randn(N, TM, D)\n",
        "tgt_mask = torch.randn(T, T) < 0.5\n",
        "\n",
        "output = decoder_layer(tgt, memory, tgt_mask)\n",
        "\n",
        "expected_output = np.asarray([\n",
        "    [[ 1.1464597, -0.32541496,  0.39171425, -0.39425734,  0.62471056,\n",
        "      -1.8665842, -0.12977494, -1.6609063, -0.5620399,  0.45006236,\n",
        "       1.6086785,  0.7173523],\n",
        "     [-0.6703264,  0.34731007, -0.01452054, -0.0500976,  0.9617562,\n",
        "      -0.91788256,  0.5138556, -1.5247818,  2.0940537, -1.0386938,\n",
        "       1.0333964, -0.7340692],\n",
        "     [-1.1966342,  0.78882384,  0.1765188,  0.04164891,  1.9480462,\n",
        "      -0.94358695,  0.83423877, -0.44660965,  1.1469632, -1.6658922,\n",
        "      -0.27915588, -0.4043607],\n",
        "     [-0.96863323,  0.10736976, -0.18560877, -0.86474127, -0.12873,\n",
        "       0.36593518,  0.9634492, -0.9432319,  1.4652547,  1.2200648,\n",
        "       0.9218512, -1.9529796]]\n",
        "])\n",
        "print('error: ', rel_error(expected_output, output.detach().numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxBcIdRT7vvz"
      },
      "source": [
        "# Transformer for Image Captioning\n",
        "Now that you have implemented the previous layers, you can combine them to build a Transformer-based image captioning model. Open the file `comp540/classifiers/transformer.py` and look at the `CaptioningTransformer` class.\n",
        "\n",
        "Implement the `forward` function of the class. After doing so, run the following to check your forward pass using a small test case; you should see error on the order of `e-5` or less."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3Vxnysk72q6"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(231)\n",
        "np.random.seed(231)\n",
        "\n",
        "N, D, W = 4, 20, 30\n",
        "word_to_idx = {'<NULL>': 0, 'cat': 2, 'dog': 3}\n",
        "V = len(word_to_idx)\n",
        "T = 3\n",
        "\n",
        "transformer = CaptioningTransformer(\n",
        "    word_to_idx,\n",
        "    input_dim=D,\n",
        "    wordvec_dim=W,\n",
        "    num_heads=2,\n",
        "    num_layers=2,\n",
        "    max_length=30\n",
        ")\n",
        "\n",
        "features = torch.randn(N, D)\n",
        "captions = torch.randint(0, V, (N, T))\n",
        "\n",
        "scores = transformer(features, captions)\n",
        "expected_scores = np.asarray([\n",
        "    [[ 0.48119992, -0.24859881, -0.7489549 ],\n",
        "     [ 0.20380056,  0.08959456, -0.89954275],\n",
        "     [ 0.21135767, -0.17083111, -0.62508506]],\n",
        "\n",
        "    [[ 0.49413955, -0.50489324, -0.79341394],\n",
        "     [ 0.87452495, -0.4392967 , -1.1513498 ],\n",
        "     [ 0.2547267 , -0.26321974, -0.93643296]],\n",
        "\n",
        "    [[ 0.70437765, -0.5729916 , -0.7946507 ],\n",
        "     [ 0.18345363, -0.31752932, -1.7304884 ],\n",
        "     [ 0.61473167, -0.82634443, -1.2179294 ]],\n",
        "\n",
        "    [[ 0.5163983 , -0.7899667 , -1.0383208 ],\n",
        "     [ 0.28063023, -0.3603301 , -1.5435203 ],\n",
        "     [ 0.7222998 , -0.71457165, -0.76669186]]\n",
        "])\n",
        "\n",
        "print('scores error: ', rel_error(expected_scores, scores.detach().numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwUhSxEx7qyv"
      },
      "source": [
        "# Overfit Transformer Captioning Model on Small Data\n",
        "Run the following to overfit the Transformer-based captioning model on a small dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HMqJq4T7qyv"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(231)\n",
        "np.random.seed(231)\n",
        "\n",
        "data = load_coco_data(max_train=50)\n",
        "\n",
        "transformer = CaptioningTransformer(\n",
        "          word_to_idx=data['word_to_idx'],\n",
        "          input_dim=data['train_features'].shape[1],\n",
        "          wordvec_dim=256,\n",
        "          num_heads=2,\n",
        "          num_layers=2,\n",
        "          max_length=30\n",
        "        )\n",
        "\n",
        "\n",
        "transformer_solver = CaptioningSolverTransformer(transformer, data, idx_to_word=data['idx_to_word'],\n",
        "           num_epochs=100,\n",
        "           batch_size=25,\n",
        "           learning_rate=0.001,\n",
        "           verbose=True, print_every=10,\n",
        "         )\n",
        "\n",
        "transformer_solver.train()\n",
        "\n",
        "# Plot the training losses.\n",
        "plt.plot(transformer_solver.loss_history)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training loss history')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcRNyolJ7qyw"
      },
      "source": [
        "Print final training loss. You should see a final loss of less than 0.05 ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPU95Nv27qyx",
        "tags": [],
        "test": "transformer_final_training_loss"
      },
      "outputs": [],
      "source": [
        "print('Final loss: ', transformer_solver.loss_history[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7R-SUFxf7qyx"
      },
      "source": [
        "# Transformer Sampling at Test Time\n",
        "The sampling code has been written for you.  The training results should be much better than the validation set results, given how little data we trained on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4uQMkIC7qyy",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# If you get an error, the URL just no longer exists, so don't worry!\n",
        "# You can re-sample as many times as you want.\n",
        "for split in ['train', 'val']:\n",
        "    minibatch = sample_coco_minibatch(data, split=split, batch_size=2)\n",
        "    gt_captions, features, urls = minibatch\n",
        "    gt_captions = decode_captions(gt_captions, data['idx_to_word'])\n",
        "\n",
        "    sample_captions = transformer.sample(features, max_length=30)\n",
        "    sample_captions = decode_captions(sample_captions, data['idx_to_word'])\n",
        "\n",
        "    for gt_caption, sample_caption, url in zip(gt_captions, sample_captions, urls):\n",
        "        img = image_from_url(url)\n",
        "        # Skip missing URLs.\n",
        "        if img is None: continue\n",
        "        plt.imshow(img)\n",
        "        plt.title('%s\\n%s\\nGT:%s' % (split, sample_caption, gt_caption))\n",
        "        plt.axis('off')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZMSyzMWyQpK"
      },
      "source": [
        "# Vision Transformer (ViT)\n",
        "\n",
        "[Dosovitskiy et. al.](https://arxiv.org/abs/2010.11929) showed that applying a transformer model on a sequence of image patches (referred to as Vision Transformer) not only achieves impressive performance but also scales more effectively than convolutional neural networks when trained on large datasets. We will build a version of Vision Transformer using our existing implementation of transformer components and train it on the CIFAR-10 dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAntTB4F0yHC"
      },
      "source": [
        "Vision Transformer converts input image into a sequence of patches of fixed size and embed each patch into a latent vector. In `comp540/transformer_layers.py`, complete the implementation of `PatchEmbedding` and test it below. You should see relative error less than 1e-4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRiRu3pN7qyz"
      },
      "outputs": [],
      "source": [
        "from comp540.transformer_layers import PatchEmbedding\n",
        "\n",
        "torch.manual_seed(231)\n",
        "np.random.seed(231)\n",
        "\n",
        "N = 2\n",
        "HW = 16\n",
        "PS = 8\n",
        "D = 8\n",
        "\n",
        "patch_embedding = PatchEmbedding(\n",
        "    img_size=HW,\n",
        "    patch_size=PS,\n",
        "    embed_dim=D\n",
        ")\n",
        "\n",
        "x = torch.randn(N, 3, HW, HW)\n",
        "output = patch_embedding(x)\n",
        "\n",
        "\n",
        "expected_output = np.asarray([\n",
        "        [[-0.6312704 ,  0.02531429,  0.6112642 , -0.49089882,\n",
        "          0.01412961, -0.6959372 , -0.32862484, -0.45402682],\n",
        "        [ 0.18816411, -0.08142513, -0.9829535 , -0.23975623,\n",
        "         -0.23109074,  0.97950286, -0.40997326,  0.7457837 ],\n",
        "        [ 0.01810865,  0.15780598, -0.91804236,  0.36185235,\n",
        "          0.8379501 ,  1.0191797 , -0.29667392,  0.20322265],\n",
        "        [-0.18697818, -0.45137224, -0.40339014, -1.4381214 ,\n",
        "         -0.43450755,  0.7651071 , -0.83683825, -0.16360264]],\n",
        "\n",
        "       [[-0.39786366,  0.16201034, -0.19008337, -1.0602452 ,\n",
        "         -0.28693503,  0.09791763,  0.26614824,  0.41781986],\n",
        "        [ 0.35146567, -0.4469593 , -0.1841726 ,  0.45757473,\n",
        "         -0.61304873, -0.29104248, -0.16124889, -0.14987172],\n",
        "        [-0.2996967 ,  0.27353522, -0.09929767,  0.01973832,\n",
        "         -1.2312065 , -0.6374332 , -0.22963578,  0.55696607],\n",
        "        [-0.93818814,  0.02465284, -0.21117875,  1.1860403 ,\n",
        "         -0.06137538, -0.21062079, -0.094347  ,  0.50032747]]])\n",
        "\n",
        "print('error: ', rel_error(expected_output, output.detach().numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-equGLTX7rxV"
      },
      "source": [
        "The sequence of patch vectors is processed by transformer encoder layers, each consisting of a self-attention and a feed-forward module. Since all vectors attend to one another, attention masking is not strictly necessary. However, we still implement it for the sake of consistency.\n",
        "\n",
        "Implement `TransformerEncoderLayer` in `comp540/transformer_layers.py` and test it below. You should see relative error less than 1e-6.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YIr_Fxv5xvy"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(231)\n",
        "np.random.seed(231)\n",
        "\n",
        "from comp540.transformer_layers import TransformerEncoderLayer\n",
        "\n",
        "N, T, TM, D = 1, 4, 5, 12\n",
        "\n",
        "encoder_layer = TransformerEncoderLayer(D, 2, 4*D)\n",
        "x = torch.randn(N, T, D)\n",
        "x_mask = torch.randn(T, T) < 0.5\n",
        "\n",
        "output = encoder_layer(x, x_mask)\n",
        "\n",
        "expected_output = np.asarray([\n",
        "    [[-0.43529928, -0.204897, 0.45693663, -1.1355408, 1.8000772,\n",
        "      0.24467856, 0.8525885, -0.53586316, -1.5606489, -1.207276,\n",
        "      1.3986266, 0.3266182],\n",
        "     [0.06928468, 1.1030475, -0.9902548, -0.34333378, -2.1073136,\n",
        "      1.1960536, 0.16573538, -1.1772276, 1.2644588, -0.27311313,\n",
        "      0.29650143, 0.7961618],\n",
        "     [0.28310525, 0.69066685, -1.2264299, 1.0175265, -2.0517688,\n",
        "     -0.10330413, -0.5355796, -0.2696466, 0.13948536, 2.0408154,\n",
        "      0.27095756, -0.25582793],\n",
        "     [-0.58568114, 0.8019579, -0.9128079, -1.6816932, 1.1572194,\n",
        "      0.39162305, 0.58195484, 0.7043353, -1.27042, -1.1870497,\n",
        "      0.9784279, 1.0221335]]\n",
        "])\n",
        "\n",
        "print('error: ', rel_error(expected_output, output.detach().numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1msDyvLseyI"
      },
      "source": [
        "Take a look at the `VisionTransformer` implementation in `comp540/classifiers/transformer.py`.\n",
        "\n",
        "For classification, ViT divides the input image into patches and processes the sequence of patch vectors using a transformer. Finally, all the patch vectors are average-pooled and used to predict the image class. We will use the same 1D sinusoidal positional encoding to inject ordering information, though 2D sinusoidal and learned positional encodings are also valid choices.\n",
        "\n",
        "Complete the ViT forward pass and test it below. You should see relative error less than 1e-6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHZ1LMuRpOiV"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(231)\n",
        "np.random.seed(231)\n",
        "from comp540.classifiers.transformer import VisionTransformer\n",
        "\n",
        "imgs = torch.randn(3, 3, 32, 32)\n",
        "transformer = VisionTransformer()\n",
        "scores = transformer(imgs)\n",
        "expected_scores = np.asarray(\n",
        "    [[-0.13013132,  0.13652277, -0.04656096, -0.16443546, -0.08946665,\n",
        "        -0.10123537,  0.11047452,  0.01317241,  0.17256221,  0.16230097],\n",
        "       [-0.11988413,  0.20006064, -0.04028708, -0.06937674, -0.07828291,\n",
        "        -0.13545093,  0.18698244,  0.01878054,  0.14309685,  0.03245382],\n",
        "       [-0.11540816,  0.21416159, -0.07740889, -0.08336161, -0.1645808 ,\n",
        "        -0.12318538,  0.18035144,  0.05492767,  0.15997584,  0.12134959]])\n",
        "print('scores error: ', rel_error(expected_scores, scores.detach().numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Redma2A5BZA"
      },
      "source": [
        "\n",
        "We will first verify our implementation by overfitting it on one training batch. Tune learning rate and weight decay accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Myf9A3UK4TpM"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from tqdm.auto import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_data = CIFAR10(root='data', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_data = CIFAR10(root='data', train=False, transform=transforms.ToTensor(), download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YU2pQigB9kxp"
      },
      "outputs": [],
      "source": [
        "\n",
        "learning_rate = 1e-4  # Experiment with this\n",
        "weight_decay = 1.e-4  # Experiment with this\n",
        "\n",
        "\n",
        "batch = next(iter(DataLoader(train_data, batch_size=64, shuffle=False)))\n",
        "model = VisionTransformer(dropout=0.0)\n",
        "loss_criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "model.train()\n",
        "\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    imgs, target = batch\n",
        "    out = model(imgs)\n",
        "    loss = loss_criterion(out, target)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    top1 = (out.argmax(-1) == target).float().mean().item()\n",
        "    if epoch % 10 == 0:\n",
        "      print(f\"[{epoch}/{epochs}] Loss {loss.item():.6f}, Top-1 Accuracy: {top1:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mrgWcfEE8XE",
        "test": "vit_overfit_accuracy"
      },
      "outputs": [],
      "source": [
        "# You should get perfect 1.00 accuracy\n",
        "print(f\"Overfitting ViT on one batch. Top-1 accuracy: {top1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oI5-AVJNGYS9"
      },
      "source": [
        "Now we will train it on the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9W2dUKrz8MbG"
      },
      "outputs": [],
      "source": [
        "from comp540.classification_solver_vit import ClassificationSolverViT\n",
        "\n",
        "############################################################################\n",
        "# TODO: Train a Vision Transformer model that achieves over 0.45 test      #\n",
        "# accuracy on CIFAR-10 after 2 epochs by adjusting the model architecture  #\n",
        "# and/or training parameters as needed.                                    #\n",
        "#                                                                          #\n",
        "# Note: If you want to use a GPU runtime, go to `Runtime > Change runtime  #\n",
        "# type` and set `Hardware accelerator` to `GPU`. This will reset Colab,    #\n",
        "# so make sure to rerun the entire notebook from the beginning afterward.  #\n",
        "############################################################################\n",
        "\n",
        "\n",
        "learning_rate = 1e-4\n",
        "weight_decay = 0.0\n",
        "batch_size = 64\n",
        "model = VisionTransformer()  # You may want to change the default params.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#                                 END OF YOUR CODE                             #\n",
        "################################################################################\n",
        "\n",
        "solver = ClassificationSolverViT(\n",
        "    train_data=train_data,\n",
        "    test_data=test_data,\n",
        "    model=model,\n",
        "    num_epochs = 2,  # Don't change this\n",
        "    learning_rate = learning_rate,\n",
        "    weight_decay = weight_decay,\n",
        "    batch_size = batch_size,\n",
        ")\n",
        "\n",
        "solver.train('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1xe22cuGsnw",
        "test": "vit_test_accuracy"
      },
      "outputs": [],
      "source": [
        "print(f\"Accuracy on test set: {solver.results['best_test_acc']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtjQqsDBm9or"
      },
      "source": [
        "# Inline Question 2\n",
        "\n",
        "Despite their recent success in large-scale image recognition tasks, ViTs often lag behind traditional CNNs when trained on smaller datasets. What underlying factor contribute to this performance gap? What techniques can be used to improve the performance of ViTs on small datasets?\n",
        "\n",
        "**Your Answer**: fill this in.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cau-UmPBJ3ez"
      },
      "source": [
        "# Inline Question 3\n",
        "\n",
        "How does the computational cost of the self-attention layers in a ViT change if we independently make the following changes? Please ignore the computation cost of QKV and output projection.\n",
        "\n",
        "(i) Double the hidden dimension.\n",
        "(ii) Double the height and width of the input image.\n",
        "(iii) Double the patch size.\n",
        "(iv) Double the number of layers.\n",
        "\n",
        "**Your Answer**: fill this in.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CB5dmapnJnQs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}